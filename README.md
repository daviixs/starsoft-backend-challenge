# ğŸ¬ Cinema Booking System â€” Backend Challenge

![Node.js](https://img.shields.io/badge/Node.js-18+-339933?style=for-the-badge&logo=nodedotjs&logoColor=white)
![NestJS](https://img.shields.io/badge/NestJS-11-E0234E?style=for-the-badge&logo=nestjs&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Redis](https://img.shields.io/badge/Redis-7-DC382D?style=for-the-badge&logo=redis&logoColor=white)
![Kafka](https://img.shields.io/badge/Apache_Kafka-7.5-231F20?style=for-the-badge&logo=apachekafka&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178C6?style=for-the-badge&logo=typescript&logoColor=white)

> Sistema de venda de ingressos de cinema com **proteÃ§Ã£o contra concorrÃªncia**, **locks distribuÃ­dos** e **mensageria assÃ­ncrona**. Projetado para rodar em mÃºltiplas instÃ¢ncias simultÃ¢neas sem race conditions, deadlocks ou reservas duplicadas.

---

## ğŸ“‘ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Diagramas da SoluÃ§Ã£o](#-diagramas-da-soluÃ§Ã£o)
- [Tecnologias Escolhidas](#-tecnologias-escolhidas)
- [Como Executar](#-como-executar)
- [Endpoints da API](#-endpoints-da-api)
- [EstratÃ©gias de ConcorrÃªncia](#-estratÃ©gias-de-concorrÃªncia)
  - [SoluÃ§Ã£o para Race Conditions](#91-soluÃ§Ã£o-para-race-conditions-condiÃ§Ã£o-de-corrida)
  - [CoordenaÃ§Ã£o entre InstÃ¢ncias](#92-coordenaÃ§Ã£o-entre-instÃ¢ncias)
  - [PrevenÃ§Ã£o de Deadlocks](#93-prevenÃ§Ã£o-de-deadlocks)
- [Arquitetura Detalhada](#-arquitetura-detalhada)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [DecisÃµes TÃ©cnicas](#-decisÃµes-tÃ©cnicas)
- [LimitaÃ§Ãµes Conhecidas](#-limitaÃ§Ãµes-conhecidas)
- [Melhorias Futuras](#-melhorias-futuras)
- [Testes](#-testes)
- [Troubleshooting](#-troubleshooting)
- [Contribuindo](#-contribuindo)
- [LicenÃ§a](#-licenÃ§a)
- [Autor](#-autor)

---

## ğŸ”­ VisÃ£o Geral

### Contexto

Este projeto foi desenvolvido como **desafio tÃ©cnico para vaga de Desenvolvedor Back-End**. O cenÃ¡rio simula uma plataforma real de venda de ingressos onde **milhares de usuÃ¡rios concorrem pelos mesmos assentos simultaneamente**.

### O Problema

Em sistemas de reserva, o principal desafio Ã© garantir que:

- **Apenas um usuÃ¡rio** consiga reservar um assento especÃ­fico, mesmo com requisiÃ§Ãµes simultÃ¢neas
- **Nenhum dinheiro seja cobrado** sem uma reserva vÃ¡lida
- **Reservas abandonadas** liberem os assentos automaticamente
- O sistema funcione corretamente com **mÃºltiplas instÃ¢ncias** da API

### Diagrama de Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Clientes  â”‚     â”‚              Docker Network                 â”‚
â”‚  (Browser/  â”‚     â”‚                                             â”‚
â”‚   Mobile)   â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚             â”œâ”€â”€â”€â”€â–ºâ”‚  â”‚  API Inst 1  â”‚    â”‚  API Inst 2  â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  :3000       â”‚    â”‚  :3001       â”‚       â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                    â”‚         â”‚                   â”‚               â”‚
                    â”‚         â–¼                   â–¼               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                    â”‚  â”‚ PostgreSQL  â”‚    â”‚    Redis     â”‚        â”‚
                    â”‚  â”‚   :5432     â”‚    â”‚   :6379      â”‚        â”‚
                    â”‚  â”‚ (ACID/Data) â”‚    â”‚ (Locks/Cache)â”‚        â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                    â”‚                                             â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                    â”‚  â”‚  Zookeeper  â”‚â”€â”€â”€â–ºâ”‚    Kafka     â”‚        â”‚
                    â”‚  â”‚   :2181     â”‚    â”‚   :9092      â”‚        â”‚
                    â”‚  â”‚             â”‚    â”‚  (Eventos)   â”‚        â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Diagramas da SoluÃ§Ã£o

### Fluxo Simplificado de Compra

```mermaid
flowchart TD
    A(["Inicio"]) --> B["Usuario Consulta Sessao"]
    B --> C["Visualiza Mapa de Assentos"]
    C --> D["Seleciona Assentos Disponiveis"]
    D --> E["Solicita Reserva"]
    E --> F{"Redis: Assentos Livres?"}
    F -- Nao --> G["Erro 409: Conflict"]
    F -- Sim --> H["Adquire Lock no Redis"]
    H --> I{"PostgreSQL: SELECT FOR UPDATE"}
    I -- Disponiveis --> J["Cria Reserva no PostgreSQL"]
    I -- Indisponiveis --> K["Rollback + Libera Locks"]
    K --> G
    J --> L["Publica Evento no Kafka"]
    L --> M["Retorna Sucesso ao Usuario"]
    M --> N{"Pagamento em 30s?"}
    N -- Sim --> O["Confirma Pagamento"]
    O --> P["Assentos marcados como SOLD"]
    P --> Q(["Fim: Ingressos Emitidos"])
    N -- Nao --> R["Worker Expira Reserva"]
    R --> S["Assentos voltam a AVAILABLE"]
    S --> C
    G --> C
```

### Arquitetura de Infraestrutura (Cluster)

```mermaid
graph TD
    Client["Cliente / Teste de Stress"] -->|"HTTP Request"| LB["Load Balancer"]

    subgraph Docker Cluster
        LB -->|"Round Robin"| App1["API Instance 1 :3000"]
        LB -->|"Round Robin"| App2["API Instance 2 :3001"]

        App1 -->|"Locks e Cache"| Redis[("Redis")]
        App2 -->|"Locks e Cache"| Redis
        App1 -->|"Persistencia ACID"| DB[("PostgreSQL")]
        App2 -->|"Persistencia ACID"| DB
        App1 -->|"Publicacao de Eventos"| Kafka{{"Kafka"}}
        App2 -->|"Publicacao de Eventos"| Kafka

        Kafka -->|"Expiry Worker"| App1
        Kafka -->|"Expiry Worker"| App2
    end
```

---

### Fluxo de ResoluÃ§Ã£o de Conflito (Race Condition)

```mermaid
flowchart TD
    U1(["Usuario A"]) --> R1["POST /reserve Assento A1"]
    U2(["Usuario B"]) --> R2["POST /reserve Assento A1"]

    R1 --> C1{"Redis: SET NX seat lock A1"}
    R2 --> C2{"Redis: SET NX seat lock A1"}

    C1 -- "OK lock" --> T1["Abre Transacao PostgreSQL"]
    C2 -- "FAIL" --> E2["409 Conflict: Seat Unavailable"]

    T1 --> S1["SELECT FOR UPDATE seats WHERE A1"]
    S1 --> U3["UPDATE status = reserved"]
    U3 --> I1["INSERT INTO reservations"]
    I1 --> CM1["COMMIT"]
    CM1 --> K1["Kafka: booking.reserved"]
    K1 --> RL1["Redis: DEL seat lock A1"]
    RL1 --> OK1(["201 Created"])

    style E2 fill:#ff6b6b,color:#fff
    style OK1 fill:#51cf66,color:#fff
```

### Fluxo Anti-Deadlock (OrdenaÃ§Ã£o de Locks)

```mermaid
flowchart LR
    subgraph UA["Usuario A quer B2, A1"]
        A1["Input: B2, A1"] --> A2["Sort para A1, B2"]
        A2 --> A3["Lock A1 OK"]
        A3 --> A4["Lock B2 OK"]
        A4 --> A5(["Reserva OK"])
    end

    subgraph UB["Usuario B quer A1, B2"]
        B1["Input: A1, B2"] --> B2["Sort para A1, B2"]
        B2 --> B3["Lock A1 aguarda A"]
        B3 --> B4["Retry / 409 Conflict"]
    end

    style A5 fill:#51cf66,color:#fff
    style B4 fill:#ff6b6b,color:#fff
```

## ğŸ› ï¸ Tecnologias Escolhidas

| Tecnologia         | VersÃ£o | Alternativas Consideradas | Por Que Escolhemos                                                                             |
| :----------------- | :----: | :------------------------ | :--------------------------------------------------------------------------------------------- |
| **PostgreSQL**     |   15   | MongoDB, MySQL            | Compliance ACID, Row-Level Locking, `SELECT FOR UPDATE`, suporte a arrays UUID                 |
| **Redis**          |   7    | Memcached, Zookeeper      | `SET NX PX` atÃ´mico para locks, TTL automÃ¡tico, latÃªncia < 1ms, cache de idempotÃªncia          |
| **Apache Kafka**   |  7.5   | RabbitMQ, AWS SQS         | Durabilidade de mensagens, replay de eventos, auditoria completa, alto throughput              |
| **NestJS**         |   11   | Express puro, Fastify     | InjeÃ§Ã£o de dependÃªncia nativa, TypeScript-first, arquitetura modular, Swagger integrado        |
| **TypeORM**        |  0.3   | Prisma, Sequelize         | Decorators nativos para entidades, `QueryRunner` para transaÃ§Ãµes, integraÃ§Ã£o direta com NestJS |
| **Docker Compose** |   â€”    | Kubernetes, bare-metal    | Setup simplificado, reprodutibilidade, orquestraÃ§Ã£o local de todos os serviÃ§os                 |

---

## ğŸš€ Como Executar

### 5.1 PrÃ©-requisitos

| Ferramenta                   | VersÃ£o MÃ­nima |
| :--------------------------- | :------------ |
| Docker                       | 20+           |
| Docker Compose               | 2+            |
| Node.js _(apenas dev local)_ | 18+           |
| Git                          | 2+            |

### 5.2 InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/starsoft-backend-challenge.git
cd starsoft-backend-challenge

# Subir todos os serviÃ§os (PostgreSQL, Redis, Kafka, 2x API)
docker-compose up --build -d
```

> â± A primeira build pode levar ~2 minutos. Aguarde os health checks passarem.

### 5.3 Verificar SaÃºde

```bash
# Health check completo (DB + Redis)
curl http://localhost:3000/api/health
```

**Resposta esperada:**

```json
{
  "status": "ok",
  "info": {
    "database": { "status": "up" },
    "redis": { "status": "up" }
  }
}
```

### 5.4 Popular Dados Iniciais

```bash
# Cria uma sessÃ£o de cinema com 24 assentos
bash scripts/seed-data.sh
```

### 5.5 Swagger / DocumentaÃ§Ã£o Interativa

Acesse a documentaÃ§Ã£o interativa da API:

```
http://localhost:3000/api/docs
```

---

## ğŸ“¡ Endpoints da API

Todos os endpoints utilizam o prefixo `/api`.

### ğŸ“Œ POST `/api/sessions` â€” Criar SessÃ£o de Cinema

Cria uma sessÃ£o com assentos gerados automaticamente (formato A1, A2, ..., B1, B2, ...).

**Request:**

```bash
curl -X POST http://localhost:3000/api/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "movieName": "Oppenheimer",
    "roomNumber": 1,
    "startsAt": "2026-02-15T19:00:00Z",
    "priceCents": 2500,
    "totalSeats": 24
  }'
```

| Campo        | Tipo     | Regras      | DescriÃ§Ã£o                           |
| :----------- | :------- | :---------- | :---------------------------------- |
| `movieName`  | string   | 1â€“255 chars | Nome do filme                       |
| `roomNumber` | int      | 1â€“20        | NÃºmero da sala                      |
| `startsAt`   | ISO 8601 | futuro      | Data/hora da sessÃ£o                 |
| `priceCents` | int      | â‰¥ 1000      | PreÃ§o em centavos (2500 = R$ 25,00) |
| `totalSeats` | int      | 16â€“100      | Quantidade de assentos              |

**Response `201`:**

```json
{
  "id": "7527e3a4-e170-4ac5-97a2-061fd1fe421e",
  "movieName": "Oppenheimer",
  "roomNumber": 1,
  "startsAt": "2026-02-15T19:00:00.000Z",
  "priceCents": 2500,
  "createdAt": "2026-02-09T21:50:00.000Z"
}
```

---

### ğŸ“Œ GET `/api/sessions` â€” Listar SessÃµes

```bash
curl http://localhost:3000/api/sessions
```

**Response `200`:**

```json
[
  {
    "id": "7527e3a4-e170-4ac5-97a2-061fd1fe421e",
    "movieName": "Oppenheimer",
    "roomNumber": 1,
    "startsAt": "2026-02-15T19:00:00.000Z",
    "priceCents": 2500,
    "createdAt": "2026-02-09T21:50:00.000Z"
  }
]
```

---

### ğŸ“Œ GET `/api/sessions/:id/availability` â€” Disponibilidade em Tempo Real

Retorna o estado de todos os assentos: disponÃ­veis, reservados e vendidos.

```bash
curl http://localhost:3000/api/sessions/7527e3a4-e170-4ac5-97a2-061fd1fe421e/availability
```

**Response `200`:**

```json
{
  "session": {
    "id": "7527e3a4-...",
    "movieName": "Oppenheimer",
    "roomNumber": 1,
    "startsAt": "2026-02-15T19:00:00.000Z",
    "priceCents": 2500
  },
  "availableSeats": ["A1", "A2", "A3", "B1", "B2", "B3"],
  "reservedSeats": ["C1"],
  "soldSeats": ["D1", "D2"]
}
```

| Status | DescriÃ§Ã£o                                 |
| :----- | :---------------------------------------- |
| `404`  | `{ "message": "Session {id} not found" }` |

---

### ğŸ“Œ POST `/api/bookings/reserve` â€” Reservar Assentos

Reserva assentos com **lock distribuÃ­do** + **transaÃ§Ã£o PostgreSQL**. A reserva expira em **30 segundos** se o pagamento nÃ£o for confirmado.

**Request:**

```bash
curl -X POST http://localhost:3000/api/bookings/reserve \
  -H "Content-Type: application/json" \
  -d '{
    "userId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "sessionId": "7527e3a4-e170-4ac5-97a2-061fd1fe421e",
    "seatNumbers": ["A1", "A2"]
  }'
```

| Campo         | Tipo     | Regras                             | DescriÃ§Ã£o          |
| :------------ | :------- | :--------------------------------- | :----------------- |
| `userId`      | UUID v4  | obrigatÃ³rio                        | ID do usuÃ¡rio      |
| `sessionId`   | UUID v4  | obrigatÃ³rio                        | ID da sessÃ£o       |
| `seatNumbers` | string[] | 1â€“10 itens, formato `[A-H]\d{1,2}` | Assentos desejados |

**Response `201`:**

```json
{
  "id": "res-uuid-...",
  "userId": "a1b2c3d4-...",
  "sessionId": "7527e3a4-...",
  "seatIds": ["seat-uuid-1", "seat-uuid-2"],
  "expiresAt": "2026-02-09T22:30:30.000Z",
  "status": "pending",
  "message": "Reservation confirmed. Complete payment within 30 seconds."
}
```

| Status | DescriÃ§Ã£o                                          |
| :----- | :------------------------------------------------- |
| `409`  | Assento(s) indisponÃ­vel(is) â€” jÃ¡ reservado/vendido |
| `404`  | SessÃ£o nÃ£o encontrada                              |
| `429`  | Rate limit excedido (mÃ¡x 5 reservas/minuto por IP) |

---

### ğŸ“Œ POST `/api/bookings/confirm` â€” Confirmar Pagamento

Converte uma reserva pendente em venda definitiva. Os assentos passam de `reserved` para `sold`.

**Request:**

```bash
curl -X POST http://localhost:3000/api/bookings/confirm \
  -H "Content-Type: application/json" \
  -d '{
    "reservationId": "res-uuid-...",
    "userId": "a1b2c3d4-..."
  }'
```

| Campo            | Tipo    | Regras      | DescriÃ§Ã£o                           |
| :--------------- | :------ | :---------- | :---------------------------------- |
| `reservationId`  | UUID v4 | obrigatÃ³rio | ID da reserva                       |
| `userId`         | UUID v4 | obrigatÃ³rio | ID do usuÃ¡rio que fez a reserva     |
| `idempotencyKey` | string  | opcional    | Chave para evitar dupla confirmaÃ§Ã£o |

**Response `201`:**

```json
{
  "id": "sale-uuid-...",
  "reservationId": "res-uuid-...",
  "userId": "a1b2c3d4-...",
  "totalAmountCents": 5000,
  "confirmedAt": "2026-02-09T22:30:15.000Z",
  "message": "Payment confirmed successfully!"
}
```

| Status | DescriÃ§Ã£o                                 |
| :----- | :---------------------------------------- |
| `404`  | Reserva nÃ£o encontrada                    |
| `410`  | Reserva expirada (passou dos 30 segundos) |
| `409`  | ConfirmaÃ§Ã£o jÃ¡ em andamento (lock)        |

---

### ğŸ“Œ GET `/api/bookings/user/:userId` â€” HistÃ³rico de Compras

```bash
curl http://localhost:3000/api/bookings/user/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

**Response `200`:**

```json
{
  "userId": "a1b2c3d4-...",
  "totalPurchases": 1,
  "purchases": [
    {
      "id": "sale-uuid-...",
      "totalAmountCents": 5000,
      "confirmedAt": "2026-02-09T22:30:15.000Z",
      "movie": "Oppenheimer",
      "startsAt": "2026-02-15T19:00:00.000Z",
      "seats": 2
    }
  ]
}
```

---

### ğŸ“Œ GET `/api/bookings/reservation/:id` â€” Status de Reserva

```bash
curl http://localhost:3000/api/bookings/reservation/res-uuid-...
```

**Response `200`:**

```json
{
  "id": "res-uuid-...",
  "userId": "a1b2c3d4-...",
  "sessionId": "7527e3a4-...",
  "seatIds": ["seat-uuid-1", "seat-uuid-2"],
  "expiresAt": "2026-02-09T22:30:30.000Z",
  "status": "pending",
  "isExpired": false,
  "timeRemaining": 22
}
```

---

### ğŸ“Œ GET `/api/health` â€” Health Check

```bash
curl http://localhost:3000/api/health
```

**Response `200`:**

```json
{
  "status": "ok",
  "info": {
    "database": { "status": "up" },
    "redis": { "status": "up" }
  }
}
```

---

## ğŸ” EstratÃ©gias de ConcorrÃªncia

O sistema implementa **3 camadas de proteÃ§Ã£o** para garantir integridade em um ambiente distribuÃ­do com mÃºltiplas instÃ¢ncias:

### 7.1 Camada 1 â€” Redis Distributed Lock (`SET NX PX`)

A primeira barreira Ã© um **lock distribuÃ­do via Redis**. Antes de qualquer operaÃ§Ã£o no banco, a API tenta adquirir locks exclusivos para cada assento:

```
SET seat:lock:{sessionId}:{seatNumber} {lockValue} PX 5000 NX
```

- **`NX`** â€” sÃ³ seta se a chave **nÃ£o existir** (exclusÃ£o mÃºtua)
- **`PX 5000`** â€” expira automaticamente em 5s (previne locks Ã³rfÃ£os)
- **LiberaÃ§Ã£o via Lua Script** â€” garante que apenas o dono do lock pode liberÃ¡-lo

```lua
if redis.call("get", KEYS[1]) == ARGV[1] then
  return redis.call("del", KEYS[1])
else
  return 0
end
```

**Por que Redis e nÃ£o sÃ³ PostgreSQL?** Redis opera em < 1ms. Ele rejeita requisiÃ§Ãµes duplicadas **antes** de abrir uma transaÃ§Ã£o no banco, economizando conexÃµes do pool.

---

### 7.2 Camada 2 â€” PostgreSQL Transaction + `SELECT FOR UPDATE`

Mesmo com o Redis protegendo, a transaÃ§Ã£o no PostgreSQL garante **ACID**:

```sql
BEGIN;

  SELECT * FROM seats
  WHERE session_id = $1 AND seat_number IN ($2, $3)
    AND status = 'available'
  ORDER BY seat_number ASC
  FOR UPDATE;  -- Lock pessimista na linha

  UPDATE seats SET status = 'reserved', reserved_until = $4
  WHERE id IN (...);

  INSERT INTO reservations (user_id, session_id, seat_ids, expires_at, status)
  VALUES ($5, $1, $6, $4, 'pending');

COMMIT;
```

O `FOR UPDATE` bloqueia as linhas dos assentos atÃ© o `COMMIT`, impedindo que outra transaÃ§Ã£o leia/modifique os mesmos dados.

---

### 7.3 Camada 3 â€” IdempotÃªncia com Cache

RequisiÃ§Ãµes idÃªnticas (mesmo usuÃ¡rio + mesma sessÃ£o + mesmos assentos) dentro de **30 segundos** retornam o resultado cacheado:

```
Chave: reserve:{userId}:{sessionId}:{seatNumbers sorted}
TTL:   30 segundos
```

Isso protege contra:

- Clique duplo do usuÃ¡rio
- Retry automÃ¡tico do frontend
- Timeout + reenvio

---

### Diagrama de SequÃªncia â€” Fluxo de Reserva

```mermaid
sequenceDiagram
    participant C as Cliente
    participant A as API NestJS
    participant R as Redis
    participant P as PostgreSQL
    participant K as Kafka

    C->>A: POST /bookings/reserve
    A->>R: GET idempotency cache
    R-->>A: null - nao cacheado

    A->>R: SET NX seat lock A1
    R-->>A: OK - lock adquirido
    A->>R: SET NX seat lock A2
    R-->>A: OK - lock adquirido

    A->>P: BEGIN TRANSACTION
    A->>P: SELECT ... FOR UPDATE seats A1, A2
    P-->>A: seats disponiveis
    A->>P: UPDATE seats SET status=reserved
    A->>P: INSERT INTO reservations
    A->>P: COMMIT

    A->>K: publish booking.reserved
    A->>R: SETEX idempotency cache 30s

    A->>R: DEL seat lock A1
    A->>R: DEL seat lock A2

    A-->>C: 201 Created - reserva pendente
```

---

### 7.4 Como Garantimos IdempotÃªncia

| OperaÃ§Ã£o    | Chave de IdempotÃªncia                         | TTL  |
| :---------- | :-------------------------------------------- | :--- |
| Reserva     | `reserve:{userId}:{sessionId}:{seats sorted}` | 30s  |
| ConfirmaÃ§Ã£o | `confirm:{reservationId}:{userId}`            | 300s |

AlÃ©m disso, a tabela `sales` possui constraint `UNIQUE(reservation_id)`, impedindo dupla confirmaÃ§Ã£o mesmo em caso de falha do cache.

---

## ğŸ 9. SoluÃ§Ã£o Detalhada para Problemas de ConcorrÃªncia

Esta seÃ§Ã£o explica **em profundidade** como o sistema lida com os trÃªs maiores desafios de um sistema distribuÃ­do de reservas.

---

### 9.1 SoluÃ§Ã£o para Race Conditions (CondiÃ§Ã£o de Corrida)

#### O Problema

Quando 10 usuÃ¡rios clicam em "Reservar" no **mesmo assento ao mesmo tempo**, sem proteÃ§Ã£o adequada, todos leem o assento como `available` e todos executam o `UPDATE` â€” resultando em **10 reservas para 1 Ãºnico assento**.

```
Timeline SEM proteÃ§Ã£o:

  t=0ms  UsuÃ¡rio A â†’ SELECT status FROM seats WHERE A1  â†’ 'available' âœ…
  t=1ms  UsuÃ¡rio B â†’ SELECT status FROM seats WHERE A1  â†’ 'available' âœ…  â† STALE READ!
  t=2ms  UsuÃ¡rio A â†’ UPDATE seats SET status='reserved'  â†’ OK
  t=3ms  UsuÃ¡rio B â†’ UPDATE seats SET status='reserved'  â†’ OK  â† DOUBLE BOOKING!
```

#### Nossa SoluÃ§Ã£o: Defesa em 3 Camadas

Implementamos uma estratÃ©gia de **defesa em profundidade** (defense-in-depth) onde cada camada complementa a anterior:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAMADA 1: Redis Distributed Lock (< 1ms)        â”‚
â”‚   Barreira rÃ¡pida â€” rejeita 9 de 10 requisiÃ§Ãµes         â”‚
â”‚   antes de tocar no banco de dados                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         CAMADA 2: PostgreSQL SELECT FOR UPDATE           â”‚
â”‚   Garantia ACID â€” mesmo se o Redis falhar,              â”‚
â”‚   o banco garante consistÃªncia via lock pessimista       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         CAMADA 3: Cache de IdempotÃªncia (Redis)          â”‚
â”‚   DeduplicaÃ§Ã£o â€” requisiÃ§Ãµes idÃªnticas em < 30s          â”‚
â”‚   retornam o mesmo resultado sem reprocessar             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Por que 3 camadas e nÃ£o apenas 1?

| CenÃ¡rio de falha                         |  Camada 1 (Redis Lock)   | Camada 2 (PG Transaction) | Camada 3 (IdempotÃªncia) |
| :--------------------------------------- | :----------------------: | :-----------------------: | :---------------------: |
| 10 usuÃ¡rios simultÃ¢neos no mesmo assento | âœ… 1 passa, 9 bloqueados |             â€”             |            â€”            |
| Redis cai momentaneamente                |         âŒ Falha         |  âœ… `FOR UPDATE` protege  |            â€”            |
| UsuÃ¡rio clica 2x rÃ¡pido                  |   âœ… Lock ainda ativo    |             â€”             |    âœ… Retorna cache     |
| Retry do frontend apÃ³s timeout           |            â€”             |             â€”             |    âœ… Retorna cache     |
| Lock Redis expira antes do COMMIT        |      âŒ Outro entra      | âœ… `FOR UPDATE` bloqueia  |            â€”            |

> **Escolha tÃ©cnica:** A Camada 1 (Redis) Ã© otimista e rÃ¡pida. A Camada 2 (PostgreSQL) Ã© pessimista e segura. Juntas, oferecem **velocidade + garantia ACID**.

#### Fluxo Detalhado com Race Condition

```mermaid
sequenceDiagram
    participant A as Usuario A
    participant B as Usuario B
    participant API as API NestJS
    participant R as Redis
    participant PG as PostgreSQL

    Note over A,B: Ambos clicam em Reservar Assento A1 simultaneamente

    A->>API: POST /reserve seat A1
    B->>API: POST /reserve seat A1

    API->>R: SET seat lock A1 val_A PX 5000 NX
    R-->>API: OK - A obteve o lock

    API->>R: SET seat lock A1 val_B PX 5000 NX
    R-->>API: NULL - B bloqueado

    Note over B,API: B recebe 409 Conflict IMEDIATAMENTE sem tocar no PostgreSQL

    API-->>B: 409 Seat unavailable

    API->>PG: BEGIN + SELECT FOR UPDATE WHERE seat=A1
    PG-->>API: seat A1 status=available
    API->>PG: UPDATE reserved + INSERT reservation + COMMIT

    API->>R: DEL seat lock A1
    API-->>A: 201 Created - reserva pendente
```

#### CÃ³digo que implementa a proteÃ§Ã£o (BookingsService)

```typescript
async reserveSeats(dto: ReserveSeatsDto): Promise<Reservation> {
  // CAMADA 3: IdempotÃªncia â€” retorna cache se existir
  const cached = await this.redisLock.getCache<Reservation>(idempotencyKey);
  if (cached) return cached;

  // CAMADA 1: Redis Lock â€” aquisiÃ§Ã£o em ORDEM ALFABÃ‰TICA
  const locks = await this.redisLock.acquireMultiple(lockKeys, 5000);
  if (!locks) throw new SeatUnavailableException(seatNumbers);

  try {
    // CAMADA 2: PostgreSQL â€” SELECT FOR UPDATE (lock pessimista)
    const seats = await queryRunner.manager.find(Seat, {
      where: { sessionId, seatNumber: In(sortedSeatNumbers), status: 'available' },
      lock: { mode: 'pessimistic_write' }, // â† FOR UPDATE
    });

    if (seats.length !== sortedSeatNumbers.length) {
      throw new SeatUnavailableException(unavailable);
    }
    // ... UPDATE + INSERT + COMMIT
  } finally {
    await this.redisLock.releaseMultiple(locks); // Sempre libera
  }
}
```

---

### 9.2 CoordenaÃ§Ã£o entre InstÃ¢ncias

#### O Problema

Com **2+ instÃ¢ncias** da API rodando simultaneamente (cinema-app-1 na porta 3000, cinema-app-2 na porta 3001), variÃ¡veis locais e locks em memÃ³ria **nÃ£o servem** â€” cada instÃ¢ncia vive em um processo isolado.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    cinema-app-1      â”‚    â”‚    cinema-app-2      â”‚
â”‚  lock em memÃ³ria: {} â”‚    â”‚  lock em memÃ³ria: {} â”‚
â”‚  (isolado!)          â”‚    â”‚  (isolado!)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚  Ambos acham que o seat A1 estÃ¡ livre!
           â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          PostgreSQL (compartilhado)      â”‚
    â”‚  Sem coordenaÃ§Ã£o â†’ DOUBLE BOOKING        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Nossa SoluÃ§Ã£o: Redis como Ponto Central de CoordenaÃ§Ã£o

O Redis Ã© o **Ãºnico ponto de verdade** para locks, acessÃ­vel por todas as instÃ¢ncias:

```mermaid
flowchart TD
    subgraph inst1["Instance 1 :3000"]
        A1["POST /reserve seat A1"] --> R1["Redis: SET NX seat lock A1"]
    end

    subgraph inst2["Instance 2 :3001"]
        A2["POST /reserve seat A1"] --> R2["Redis: SET NX seat lock A1"]
    end

    R1 --> Redis[("Redis - compartilhado")]
    R2 --> Redis

    Redis -->|"OK primeiro"| S1["Inst 1 prossegue"]
    Redis -->|"NULL segundo"| S2["Inst 2 bloqueada"]

    S1 --> PG[("PostgreSQL")]
    S2 --> E["409 Conflict"]

    style S1 fill:#51cf66,color:#fff
    style E fill:#ff6b6b,color:#fff
```

#### Mecanismos de CoordenaÃ§Ã£o Implementados

| Mecanismo                 | Tecnologia              | PropÃ³sito                                   | Escopo          |
| :------------------------ | :---------------------- | :------------------------------------------ | :-------------- |
| **Lock DistribuÃ­do**      | Redis `SET NX PX`       | ExclusÃ£o mÃºtua entre instÃ¢ncias             | Por assento     |
| **Lock Pessimista**       | PostgreSQL `FOR UPDATE` | ConsistÃªncia ACID final                     | Por linha no DB |
| **Cache de IdempotÃªncia** | Redis `SETEX`           | Evitar reprocessamento duplicado            | Por requisiÃ§Ã£o  |
| **Mensageria**            | Kafka topics            | Notificar todas as instÃ¢ncias sobre eventos | Global          |
| **Cron DistribuÃ­do**      | `@nestjs/schedule`      | ExpiraÃ§Ã£o de reservas (cada instÃ¢ncia roda) | Por instÃ¢ncia   |

#### Por que Redis SET NX funciona entre instÃ¢ncias?

O comando `SET key value PX ttl NX` Ã© **atÃ´mico no Redis** â€” o servidor Redis processa um comando por vez (single-threaded para operaÃ§Ãµes de dados). Quando duas instÃ¢ncias enviam `SET NX` para a mesma chave **no mesmo microssegundo**:

```
t=0.000ms  Inst 1 â†’ SET seat:lock:A1 "inst1-lock" PX 5000 NX
t=0.000ms  Inst 2 â†’ SET seat:lock:A1 "inst2-lock" PX 5000 NX

â†’ Redis serializa internamente:
  1. Processa Inst 1 â†’ chave nÃ£o existe â†’ SET â†’ retorna OK
  2. Processa Inst 2 â†’ chave JÃ existe â†’ retorna NULL

â†’ Resultado: garantia de que APENAS 1 instÃ¢ncia obtÃ©m o lock
```

#### LiberaÃ§Ã£o segura com Lua Script

O Lua script garante que **apenas o dono do lock** pode liberÃ¡-lo â€” evitando que uma instÃ¢ncia libere o lock de outra:

```lua
-- Executado atomicamente no Redis
if redis.call("get", KEYS[1]) == ARGV[1] then
  return redis.call("del", KEYS[1])   -- SÃ³ deleta se o valor bate
else
  return 0                             -- NÃ£o Ã© meu lock, nÃ£o toco
end
```

**CenÃ¡rio que isso protege:**

```
t=0ms    Inst 1 adquire lock (valor="inst1-abc")
t=4999ms Lock estÃ¡ prestes a expirar
t=5000ms Lock expira automaticamente (PX 5000)
t=5001ms Inst 2 adquire o MESMO lock (valor="inst2-xyz")
t=5002ms Inst 1 termina processamento e tenta DEL
         â†’ Lua script verifica: "inst1-abc" â‰  "inst2-xyz"
         â†’ NÃƒO deleta! Lock da Inst 2 permanece seguro âœ…
```

#### Kafka como canal de comunicaÃ§Ã£o assÃ­ncrona

O Kafka permite que eventos de uma instÃ¢ncia sejam **consumidos por todas as outras**:

```mermaid
flowchart LR
    subgraph inst1["Instancia 1"]
        A1["Reserva criada"] --> P1["Kafka Producer"]
    end

    P1 --> T["Topic: booking.reserved"]

    T --> C1["Instancia 1: Expiry Worker"]
    T --> C2["Instancia 2: Expiry Worker"]

    C1 --> S1["setTimeout expire, 30s"]
    C2 --> S2["setTimeout expire, 30s"]

    S1 --> CHECK{"Reserva ainda pendente?"}
    S2 --> CHECK
    CHECK -- Sim --> EXP["Expira reserva"]
    CHECK -- Nao --> SKIP["Ignora - ja confirmada"]
```

> **Nota:** Mesmo que ambas as instÃ¢ncias tentem expirar a mesma reserva, a operaÃ§Ã£o Ã© idempotente â€” a transaÃ§Ã£o no PostgreSQL verifica `status = 'pending'` antes de atualizar.

---

### 9.3 PrevenÃ§Ã£o de Deadlocks

#### O Problema

Um **deadlock** ocorre quando dois processos ficam travados esperando um pelo outro, indefinidamente:

```
SEM ordenaÃ§Ã£o de locks:

  Inst 1: adquire lock A1 â†’ tenta lock B2 â†’ â³ esperando Inst 2 liberar B2
  Inst 2: adquire lock B2 â†’ tenta lock A1 â†’ â³ esperando Inst 1 liberar A1

  â†’ DEADLOCK! Nenhum dos dois avanÃ§a. Sistema trava.
```

#### Nossa SoluÃ§Ã£o: Ordered Lock Acquisition (AquisiÃ§Ã£o Ordenada de Locks)

A tÃ©cnica clÃ¡ssica para prevenir deadlocks Ã© **impor uma ordem global** na aquisiÃ§Ã£o de locks. Se todos os processos adquirem locks **na mesma ordem**, Ã© impossÃ­vel formarem um ciclo de espera.

**ImplementaÃ§Ã£o:**

```typescript
// RedisLockService.acquireMultiple()
async acquireMultiple(keys: string[], ttlMs = 5000) {
  const sortedKeys = [...keys].sort(); // â† CHAVE: ordem alfabÃ©tica
  const acquiredLocks = new Map<string, string>();

  for (const key of sortedKeys) {
    const lockValue = await this.acquire(key, ttlMs);
    if (!lockValue) {
      // Se falhar em QUALQUER lock, libera TODOS os jÃ¡ adquiridos
      await this.releaseMultiple(acquiredLocks);
      return null;
    }
    acquiredLocks.set(key, lockValue);
  }
  return acquiredLocks;
}
```

#### Prova de que NÃƒO hÃ¡ deadlock

```mermaid
flowchart TD
    subgraph sem["Sem Ordenacao - DEADLOCK"]
        D1["Inst 1: lock B2 OK"] --> D2["Inst 1: lock A1 espera"]
        D3["Inst 2: lock A1 OK"] --> D4["Inst 2: lock B2 espera"]
        D2 -. "espera" .-> D3
        D4 -. "espera" .-> D1
    end

    subgraph com["Com Ordenacao - SEGURO"]
        S1["Inst 1: sort B2,A1 para A1,B2"] --> S2["Inst 1: lock A1 OK"]
        S2 --> S3["Inst 1: lock B2 OK"]
        S4["Inst 2: sort A1,B2 para A1,B2"] --> S5["Inst 2: lock A1 aguarda"]
        S5 --> S6["Inst 2: retry/timeout 409"]
    end

    style D2 fill:#ff6b6b,color:#fff
    style D4 fill:#ff6b6b,color:#fff
    style S3 fill:#51cf66,color:#fff
    style S6 fill:#ffd43b,color:#000
```

#### Mecanismo de Rollback em Falha Parcial

Se a aquisiÃ§Ã£o do **segundo lock** falha (por timeout ou lock jÃ¡ adquirido por outro), o sistema **libera todos os locks jÃ¡ adquiridos** antes de retornar erro:

```
CenÃ¡rio: UsuÃ¡rio quer reservar [A1, B2, C3]

  1. sort() â†’ [A1, B2, C3]
  2. lock(A1) â†’ OK âœ…
  3. lock(B2) â†’ OK âœ…
  4. lock(C3) â†’ FALHA âŒ (outro usuÃ¡rio jÃ¡ tem)

  â†’ Rollback automÃ¡tico:
    - release(A1) âœ…
    - release(B2) âœ…
    - Retorna null â†’ Controller lanÃ§a 409 Conflict

  â†’ Nenhum lock fica "Ã³rfÃ£o" ocupando recursos
```

#### ProteÃ§Ã£o adicional: TTL nos locks

Mesmo em cenÃ¡rios extremos (crash da aplicaÃ§Ã£o, network partition), os locks **expiram automaticamente** apÃ³s 5 segundos (`PX 5000`):

```
t=0s     Inst 1 adquire lock(A1) com TTL=5s
t=0.5s   Inst 1 CRASH! (OOM, kill -9, etc)
t=5s     Lock expira automaticamente no Redis
t=5.1s   Inst 2 consegue adquirir lock(A1) normalmente

â†’ Sistema se recupera sozinho, sem intervenÃ§Ã£o manual
```

#### Tabela resumo: todas as proteÃ§Ãµes contra deadlocks

| ProteÃ§Ã£o                      | Mecanismo                                         | Onde                                 |
| :---------------------------- | :------------------------------------------------ | :----------------------------------- |
| **OrdenaÃ§Ã£o de locks**        | `sort()` antes de `acquireMultiple()`             | `RedisLockService`                   |
| **Rollback em falha parcial** | Libera todos os locks adquiridos se um falhar     | `RedisLockService.acquireMultiple()` |
| **TTL automÃ¡tico**            | `PX 5000` em todo `SET NX`                        | Redis                                |
| **LiberaÃ§Ã£o via Lua**         | Script atÃ´mico que verifica ownership             | `RedisLockService.release()`         |
| **Retry com backoff**         | Tentativas com `delay * (attempt + 1)`            | `RedisLockService.acquire()`         |
| **OrdenaÃ§Ã£o no SQL**          | `ORDER BY seat_number ASC` no `SELECT FOR UPDATE` | `BookingsService`                    |

---

## ğŸ— Arquitetura Detalhada

### 8.1 Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Compose                           â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   cinema-app-1     â”‚        â”‚   cinema-app-2     â”‚          â”‚
â”‚  â”‚   (NestJS :3000)   â”‚        â”‚   (NestJS :3001)   â”‚          â”‚
â”‚  â”‚                    â”‚        â”‚                    â”‚          â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚
â”‚  â”‚ â”‚ Controllers  â”‚   â”‚        â”‚ â”‚ Controllers  â”‚   â”‚          â”‚
â”‚  â”‚ â”‚ Services     â”‚   â”‚        â”‚ â”‚ Services     â”‚   â”‚          â”‚
â”‚  â”‚ â”‚ Workers      â”‚   â”‚        â”‚ â”‚ Workers      â”‚   â”‚          â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                             â”‚                     â”‚
â”‚       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                â”‚
â”‚       â”‚                                       â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                â”‚
â”‚  â”‚PostgreSQLâ”‚   â”‚  Redis  â”‚   â”‚    Kafka     â”‚â”‚                â”‚
â”‚  â”‚  :5432  â”‚   â”‚  :6379  â”‚   â”‚    :9092     â”‚â”‚                â”‚
â”‚  â”‚         â”‚   â”‚         â”‚   â”‚              â”‚â”‚                â”‚
â”‚  â”‚â€¢ SessÃµesâ”‚   â”‚â€¢ Locks  â”‚   â”‚â€¢ booking.*   â”‚â”‚                â”‚
â”‚  â”‚â€¢ Seats  â”‚   â”‚â€¢ Cache  â”‚   â”‚  .reserved   â”‚â”‚                â”‚
â”‚  â”‚â€¢ Reserv.â”‚   â”‚â€¢ Idemp. â”‚   â”‚  .confirmed  â”‚â”‚                â”‚
â”‚  â”‚â€¢ Sales  â”‚   â”‚         â”‚   â”‚  .expired    â”‚â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                â”‚
â”‚                                               â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 Fluxo de Reserva (Step by Step)

1. **Cliente** â†’ `POST /api/bookings/reserve` com `{ userId, sessionId, seatNumbers }`
2. **API** â†’ Verifica se a sessÃ£o existe no PostgreSQL
3. **API** â†’ Verifica cache de idempotÃªncia no Redis
4. **API** â†’ `Redis.SET NX PX` para cada assento (ordem alfabÃ©tica)
5. **API** â†’ `PostgreSQL BEGIN TRANSACTION`
6. **API** â†’ `SELECT FOR UPDATE` nos assentos
7. **API** â†’ `UPDATE seats SET status = 'reserved'`
8. **API** â†’ `INSERT INTO reservations`
9. **API** â†’ `COMMIT`
10. **API** â†’ `Kafka.publish('booking.reserved', { ... })`
11. **API** â†’ `Redis.SETEX` cache de idempotÃªncia (30s)
12. **API** â†’ Libera locks no Redis via Lua script
13. **API** â†’ Retorna `201 { reservationId, expiresAt }`

### 8.3 Worker de ExpiraÃ§Ã£o de Reservas

O `ReservationExpiryWorker` opera em **duas frentes**:

**1. Cron Job (a cada 10 segundos):**

- Busca reservas com `status = 'pending'` e `expires_at < NOW()`
- Para cada reserva expirada:
  - `UPDATE seats SET status = 'available'`
  - `UPDATE reservations SET status = 'expired'`
  - `Kafka.publish('booking.expired', { ... })`

**2. Kafka Consumer (event-driven):**

- Escuta o tÃ³pico `booking.reserved`
- Ao receber evento, agenda `setTimeout` para verificar expiraÃ§Ã£o apÃ³s o TTL
- Garante expiraÃ§Ã£o pontual mesmo entre ciclos do cron

### 8.4 TÃ³picos Kafka

| TÃ³pico              | Evento               | Publicado Quando                 |
| :------------------ | :------------------- | :------------------------------- |
| `booking.reserved`  | Reserva criada       | ApÃ³s commit da reserva           |
| `booking.confirmed` | Pagamento confirmado | ApÃ³s commit da venda             |
| `booking.expired`   | Reserva expirada     | ApÃ³s expirar reserva pelo worker |

---

## ğŸ“ Estrutura do Projeto

```
src/
â”œâ”€â”€ main.ts                          # Bootstrap, Swagger, ValidationPipe
â”œâ”€â”€ app.module.ts                    # MÃ³dulo raiz (Config, TypeORM, Throttler)
â”œâ”€â”€ app.controller.ts                # Controller raiz
â”œâ”€â”€ app.service.ts                   # Service raiz
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database.config.ts           # ConfiguraÃ§Ã£o PostgreSQL (TypeORM)
â”‚   â”œâ”€â”€ redis.config.ts              # ConfiguraÃ§Ã£o Redis (ioredis)
â”‚   â””â”€â”€ kafka.config.ts              # ConfiguraÃ§Ã£o Kafka (kafkajs)
â”‚
â”œâ”€â”€ sessions/
â”‚   â”œâ”€â”€ sessions.module.ts           # MÃ³dulo de sessÃµes
â”‚   â”œâ”€â”€ sessions.controller.ts       # POST /sessions, GET /sessions, GET /sessions/:id/availability
â”‚   â””â”€â”€ sessions.service.ts          # LÃ³gica de criaÃ§Ã£o e consulta de sessÃµes
â”‚
â”œâ”€â”€ booking/
â”‚   â”œâ”€â”€ booking.module.ts            # MÃ³dulo de reservas
â”‚   â”œâ”€â”€ booking.controller.ts        # POST /bookings/reserve, POST /bookings/confirm, GET /bookings/user/:id
â”‚   â”œâ”€â”€ bookings.service.ts          # LÃ³gica de reserva com locks + transaÃ§Ãµes
â”‚   â””â”€â”€ use-cases/
â”‚       â””â”€â”€ reserve-seats.use-case.ts
â”‚
â”œâ”€â”€ modules/sessions/entities/
â”‚   â”œâ”€â”€ session.entity.ts            # Entidade Session (filme, sala, horÃ¡rio, preÃ§o)
â”‚   â”œâ”€â”€ seat.entity.ts               # Entidade Seat (assento, status, reservedUntil)
â”‚   â”œâ”€â”€ reservation.entity.ts        # Entidade Reservation (userId, seatIds[], expiresAt)
â”‚   â””â”€â”€ sale.entity.ts               # Entidade Sale (reservationId, totalAmountCents)
â”‚
â”œâ”€â”€ dto/
â”‚   â”œâ”€â”€ create-session.dto.ts        # DTO com validaÃ§Ã£o (class-validator)
â”‚   â”œâ”€â”€ reserve-seats.dto.ts         # DTO com regex para formato de assento
â”‚   â””â”€â”€ confirm-payment.dto.ts       # DTO com idempotencyKey opcional
â”‚
â”œâ”€â”€ redis/
â”‚   â””â”€â”€ redis-lock.service.ts        # Locks distribuÃ­dos (SET NX PX, Lua script, acquireMultiple)
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ kafka.module.ts              # MÃ³dulo Kafka
â”‚   â”œâ”€â”€ kafka.producer.service.ts    # PublicaÃ§Ã£o de eventos
â”‚   â””â”€â”€ kafka-consumer.service.ts    # Consumo de eventos
â”‚
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ reservation-expiry.worker.ts # Cron + Kafka consumer para expirar reservas
â”‚
â”œâ”€â”€ health/
â”‚   â”œâ”€â”€ health.module.ts             # MÃ³dulo de health check
â”‚   â””â”€â”€ health.controller.ts         # GET /health (DB + Redis)
â”‚
â”œâ”€â”€ exceptions/
â”‚   â””â”€â”€ business.exceptions.ts       # Exceptions customizadas (409, 404, 410)
â”‚
â””â”€â”€ shared/
    â””â”€â”€ shared.module.ts             # MÃ³dulo compartilhado (Redis, Kafka exports)

test/
â”œâ”€â”€ jest-e2e.json                    # ConfiguraÃ§Ã£o Jest E2E
â”œâ”€â”€ app.e2e-spec.ts                  # Testes E2E da aplicaÃ§Ã£o
â””â”€â”€ bookings.e2e-spec.ts             # Testes E2E de reservas e concorrÃªncia

scripts/
â”œâ”€â”€ seed-data.sh                     # Popular dados iniciais
â”œâ”€â”€ test-full-flow.sh                # Teste completo do fluxo (reserva â†’ pagamento)
â””â”€â”€ concurrency-test.sh              # Teste de concorrÃªncia (10 usuÃ¡rios simultÃ¢neos)
```

---

## ğŸ’¡ DecisÃµes TÃ©cnicas

### Por que Redis + PostgreSQL (HÃ­brido)?

| Componente     | Responsabilidade                                          | LatÃªncia |
| :------------- | :-------------------------------------------------------- | :------- |
| **Redis**      | CoordenaÃ§Ã£o rÃ¡pida entre instÃ¢ncias (locks, idempotÃªncia) | < 1ms    |
| **PostgreSQL** | PersistÃªncia ACID, garantia de consistÃªncia final         | ~5ms     |

Usar **apenas PostgreSQL** exigiria `SELECT FOR UPDATE` em todas as requisiÃ§Ãµes, sobrecarregando o pool de conexÃµes. O Redis filtra requisiÃ§Ãµes concorrentes **antes** de tocar no banco.

### Por que Kafka e nÃ£o RabbitMQ?

- **Durabilidade**: mensagens persistidas em disco com retenÃ§Ã£o configurÃ¡vel
- **Replay**: possibilidade de reprocessar eventos passados
- **Auditoria completa**: log imutÃ¡vel de todos os eventos (`reserved`, `confirmed`, `expired`)
- **Scaling**: partiÃ§Ãµes permitem consumo paralelo em mÃºltiplas instÃ¢ncias

### Por que TypeORM e nÃ£o Prisma?

- **QueryRunner**: controle total sobre transaÃ§Ãµes (`BEGIN`, `COMMIT`, `ROLLBACK`)
- **Lock pessimista**: suporte nativo a `SELECT FOR UPDATE` via `{ lock: { mode: 'pessimistic_write' } }`
- **Decorators**: entidades declarativas integradas ao ecossistema NestJS
- **Auto-sync**: `synchronize: true` para desenvolvimento rÃ¡pido

### Por que Rate Limiting com @nestjs/throttler?

- **60 req/min** por IP (default)
- **5 req/min** para rotas sensÃ­veis (reserve/confirm) â€” evita abuse de recursos
- ConfiguraÃ§Ã£o por route via decorator `@Throttle()`

---

## âš ï¸ LimitaÃ§Ãµes Conhecidas

| LimitaÃ§Ã£o                                         | Motivo                                                                                                                            |
| :------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------- |
| âŒ Dead Letter Queue (DLQ) nÃ£o implementada       | Priorizado: fluxo core primeiro. Mensagens com falha sÃ£o re-tentadas pelo Kafka, mas nÃ£o hÃ¡ fila separada para erros persistentes |
| âŒ MÃ©tricas (Prometheus/Grafana) nÃ£o configuradas | Escopo do desafio focava na lÃ³gica de concorrÃªncia                                                                                |
| âŒ Rate limiting apenas por IP                    | Uma soluÃ§Ã£o produÃ§Ã£o teria rate limit por `userId` via token JWT                                                                  |
| âŒ AutenticaÃ§Ã£o/AutorizaÃ§Ã£o nÃ£o implementada      | O `userId` Ã© passado no body. Em produÃ§Ã£o, viria de um JWT validado                                                               |
| âŒ Testes E2E com cobertura parcial               | Priorizados: cenÃ¡rios de concorrÃªncia e fluxo principal                                                                           |
| âŒ Dockerfile usa Node 18                         | Pode causar `crypto is not defined` em runtime â€” recomenda-se Node 20+                                                            |

---

## ğŸ”® Melhorias Futuras

### Curto Prazo (1â€“2 semanas)

- [ ] Atualizar Dockerfile para Node.js 20+
- [ ] Implementar autenticaÃ§Ã£o JWT
- [ ] Aumentar cobertura de testes E2E para > 80%
- [ ] Configurar CI/CD pipeline (GitHub Actions)
- [ ] Expandir Swagger com exemplos de response

### MÃ©dio Prazo (1 mÃªs)

- [ ] Dead Letter Queue para mensagens Kafka com falha
- [ ] Observabilidade completa (Prometheus + Grafana)
- [ ] Kubernetes deployment com Helm Charts
- [ ] Rate limiting por usuÃ¡rio (nÃ£o sÃ³ por IP)
- [ ] WebSocket para atualizaÃ§Ã£o em tempo real de disponibilidade

### Longo Prazo (3â€“6 meses)

- [ ] Event Sourcing completo (estado derivado de eventos)
- [ ] CQRS (Command Query Responsibility Segregation)
- [ ] Multi-tenancy (mÃºltiplas redes de cinema)
- [ ] MigraÃ§Ã£o para Redlock (Redis multi-node)
- [ ] Feature flags para rollout gradual

---

## ğŸ§ª Testes

### Testes UnitÃ¡rios

```bash
npm run test
```

### Testes E2E

```bash
# Certifique-se de que PostgreSQL, Redis e Kafka estejam rodando
docker-compose up -d postgres redis kafka zookeeper

npm run test:e2e
```

### Teste de Fluxo Completo (Manual)

```bash
# Cria sessÃ£o â†’ Reserva â†’ Conflito â†’ Pagamento â†’ Verifica histÃ³rico
bash scripts/test-full-flow.sh
```

### Teste de ConcorrÃªncia

```bash
# 1. Crie uma sessÃ£o e copie o ID
bash scripts/seed-data.sh

# 2. Rode o teste com o SESSION_ID
bash scripts/concurrency-test.sh <SESSION_ID>
```

**Resultado esperado:**

```
ğŸ“Š Results:
âœ… User #3: reservation-uuid-...
âŒ User #1: CONFLICT
âŒ User #2: CONFLICT
âŒ User #4: CONFLICT
...

ğŸ‰ TEST PASSED! Exactly 1 reservation succeeded (as expected)
```

> 10 requisiÃ§Ãµes simultÃ¢neas para o **mesmo assento** â†’ apenas **1** reserva bem-sucedida, 9 conflitos (HTTP 409).

---

## ğŸ”§ Troubleshooting

### Database "cinema" does not exist

Volume corrompido. Recrie:

```bash
docker-compose down -v
docker-compose up -d
```

### Kafka nÃ£o conecta

```bash
# Verificar logs
docker-compose logs kafka

# Reiniciar Kafka + Zookeeper
docker-compose restart zookeeper kafka
```

### PostgreSQL Connection Refused

```bash
# Verificar se estÃ¡ rodando
docker-compose ps postgres

# Acessar psql
docker exec -it cinema-postgres psql -U cinema -d cinema_booking
```

### Redis Lock Timeout

```bash
# Inspecionar locks ativos
docker exec -it cinema-redis redis-cli
> KEYS seat:lock:*
> DEL seat:lock:{sessionId}:{seatNumber}
```

### Erro `crypto is not defined` no Docker

O Dockerfile usa Node.js 18 que pode nÃ£o ter `crypto` global. Atualize para Node 20:

```dockerfile
FROM node:20-alpine AS builder
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a **MIT**. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ‘¤ Autor

**[Davi SIlva]**

- ğŸ“§ Email: xaviersilvadavi@gmail.com
- ğŸ”— LinkedIn: [seu-perfil](https://linkedin.com/in/davi-xavier-silva)

---

## ğŸ™ Agradecimentos

- Equipe **Starsoft** pela oportunidade e desafio tÃ©cnico

---

<p align="center">
  <b>Desenvolvido com â¤ï¸ e muito cafÃ© â˜•</b>
</p>
