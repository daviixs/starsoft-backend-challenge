# ğŸ¬ Cinema Booking System â€” Backend Challenge

![Node.js](https://img.shields.io/badge/Node.js-18+-339933?style=for-the-badge&logo=nodedotjs&logoColor=white)
![NestJS](https://img.shields.io/badge/NestJS-11-E0234E?style=for-the-badge&logo=nestjs&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Redis](https://img.shields.io/badge/Redis-7-DC382D?style=for-the-badge&logo=redis&logoColor=white)
![Kafka](https://img.shields.io/badge/Apache_Kafka-7.5-231F20?style=for-the-badge&logo=apachekafka&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178C6?style=for-the-badge&logo=typescript&logoColor=white)

> Sistema de venda de ingressos de cinema com **proteÃ§Ã£o contra concorrÃªncia**, **locks distribuÃ­dos** e **mensageria assÃ­ncrona**. Projetado para rodar em mÃºltiplas instÃ¢ncias simultÃ¢neas sem race conditions, deadlocks ou reservas duplicadas.

---

## ğŸ“‘ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Diagramas da SoluÃ§Ã£o](#-diagramas-da-soluÃ§Ã£o)
- [Tecnologias Escolhidas](#-tecnologias-escolhidas)
- [Como Executar](#-como-executar)
  - [Via Docker (ProduÃ§Ã£o)](#51-via-docker-produÃ§Ã£o)
  - [Via npm run start:dev (Desenvolvimento)](#52-via-npm-run-startdev-desenvolvimento-local)
- [DocumentaÃ§Ã£o da API (Swagger)](#-documentaÃ§Ã£o-da-api-swaggeropenapi)
- [Endpoints da API](#-endpoints-da-api)
- [EstratÃ©gias de ConcorrÃªncia](#-estratÃ©gias-de-concorrÃªncia)
  - [SoluÃ§Ã£o para Race Conditions](#91-soluÃ§Ã£o-para-race-conditions-condiÃ§Ã£o-de-corrida)
  - [CoordenaÃ§Ã£o entre InstÃ¢ncias](#92-coordenaÃ§Ã£o-entre-instÃ¢ncias)
  - [PrevenÃ§Ã£o de Deadlocks](#93-prevenÃ§Ã£o-de-deadlocks)
- [Arquitetura Detalhada](#-arquitetura-detalhada)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [DecisÃµes TÃ©cnicas](#-decisÃµes-tÃ©cnicas)
- [Rate Limiting](#-rate-limiting)
- [LimitaÃ§Ãµes Conhecidas](#-limitaÃ§Ãµes-conhecidas)
- [Testes](#-testes)
  - [Testes UnitÃ¡rios](#testes-unitÃ¡rios)
  - [Testes de IntegraÃ§Ã£o e ConcorrÃªncia](#testes-de-integraÃ§Ã£oconcorrÃªncia)
  - [Scripts de Teste](#scripts-de-teste)
- [Troubleshooting](#-troubleshooting)
- [Autor](#-autor)

---

## ğŸ”­ VisÃ£o Geral

### Contexto

Este projeto foi desenvolvido como **desafio tÃ©cnico para vaga de Desenvolvedor Back-End**. O cenÃ¡rio simula uma plataforma real de venda de ingressos onde **milhares de usuÃ¡rios concorrem pelos mesmos assentos simultaneamente**.

### O Problema

Em sistemas de reserva, o principal desafio Ã© garantir que:

- **Apenas um usuÃ¡rio** consiga reservar um assento especÃ­fico, mesmo com requisiÃ§Ãµes simultÃ¢neas
- **Nenhum dinheiro seja cobrado** sem uma reserva vÃ¡lida
- **Reservas abandonadas** liberem os assentos automaticamente
- O sistema funcione corretamente com **mÃºltiplas instÃ¢ncias** da API

### Diagrama de Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Clientes  â”‚     â”‚              Docker Network                 â”‚
â”‚  (Browser/  â”‚     â”‚                                             â”‚
â”‚   Mobile)   â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚             â”œâ”€â”€â”€â”€â–ºâ”‚  â”‚  API Inst 1  â”‚    â”‚  API Inst 2  â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  :3000       â”‚    â”‚  :3001       â”‚       â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                    â”‚         â”‚                   â”‚               â”‚
                    â”‚         â–¼                   â–¼               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                    â”‚  â”‚ PostgreSQL  â”‚    â”‚    Redis     â”‚        â”‚
                    â”‚  â”‚   :5432     â”‚    â”‚   :6379      â”‚        â”‚
                    â”‚  â”‚ (ACID/Data) â”‚    â”‚ (Locks/Cache)â”‚        â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                    â”‚                                             â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                    â”‚  â”‚  Zookeeper  â”‚â”€â”€â”€â–ºâ”‚    Kafka     â”‚        â”‚
                    â”‚  â”‚   :2181     â”‚    â”‚   :9092      â”‚        â”‚
                    â”‚  â”‚             â”‚    â”‚  (Eventos)   â”‚        â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Diagramas da SoluÃ§Ã£o

### Fluxo Simplificado de Compra

```mermaid
flowchart TD
    A(["Inicio"]) --> B["Usuario Consulta Sessao"]
    B --> C["Visualiza Mapa de Assentos"]
    C --> D["Seleciona Assentos Disponiveis"]
    D --> E["Solicita Reserva"]
    E --> F{"Redis: Assentos Livres?"}
    F -- Nao --> G["Erro 409: Conflict"]
    F -- Sim --> H["Adquire Lock no Redis"]
    H --> I{"PostgreSQL: SELECT FOR UPDATE"}
    I -- Disponiveis --> J["Cria Reserva no PostgreSQL"]
    I -- Indisponiveis --> K["Rollback + Libera Locks"]
    K --> G
    J --> L["Publica Evento no Kafka"]
    L --> M["Retorna Sucesso ao Usuario"]
    M --> N{"Pagamento em 30s?"}
    N -- Sim --> O["Confirma Pagamento"]
    O --> P["Assentos marcados como SOLD"]
    P --> Q(["Fim: Ingressos Emitidos"])
    N -- Nao --> R["Worker Expira Reserva"]
    R --> S["Assentos voltam a AVAILABLE"]
    S --> C
    G --> C
```

### Arquitetura de Infraestrutura (Cluster)

```mermaid
graph TD
    Client["Cliente / Teste de Stress"] -->|"HTTP Request"| LB["Load Balancer"]

    subgraph Docker Cluster
        LB -->|"Round Robin"| App1["API Instance 1 :3000"]
        LB -->|"Round Robin"| App2["API Instance 2 :3001"]

        App1 -->|"Locks e Cache"| Redis[("Redis")]
        App2 -->|"Locks e Cache"| Redis
        App1 -->|"Persistencia ACID"| DB[("PostgreSQL")]
        App2 -->|"Persistencia ACID"| DB
        App1 -->|"Publicacao de Eventos"| Kafka{{"Kafka"}}
        App2 -->|"Publicacao de Eventos"| Kafka

        Kafka -->|"Expiry Worker"| App1
        Kafka -->|"Expiry Worker"| App2
    end
```

---

### Fluxo de ResoluÃ§Ã£o de Conflito (Race Condition)

```mermaid
flowchart TD
    U1(["Usuario A"]) --> R1["POST /reserve Assento A1"]
    U2(["Usuario B"]) --> R2["POST /reserve Assento A1"]

    R1 --> C1{"Redis: SET NX seat lock A1"}
    R2 --> C2{"Redis: SET NX seat lock A1"}

    C1 -- "OK lock" --> T1["Abre Transacao PostgreSQL"]
    C2 -- "FAIL" --> E2["409 Conflict: Seat Unavailable"]

    T1 --> S1["SELECT FOR UPDATE seats WHERE A1"]
    S1 --> U3["UPDATE status = reserved"]
    U3 --> I1["INSERT INTO reservations"]
    I1 --> CM1["COMMIT"]
    CM1 --> K1["Kafka: booking.reserved"]
    K1 --> RL1["Redis: DEL seat lock A1"]
    RL1 --> OK1(["201 Created"])

    style E2 fill:#ff6b6b,color:#fff
    style OK1 fill:#51cf66,color:#fff
```

### Fluxo Anti-Deadlock (OrdenaÃ§Ã£o de Locks)

```mermaid
flowchart LR
    subgraph UA["Usuario A quer B2, A1"]
        A1["Input: B2, A1"] --> A2["Sort para A1, B2"]
        A2 --> A3["Lock A1 OK"]
        A3 --> A4["Lock B2 OK"]
        A4 --> A5(["Reserva OK"])
    end

    subgraph UB["Usuario B quer A1, B2"]
        B1["Input: A1, B2"] --> B2["Sort para A1, B2"]
        B2 --> B3["Lock A1 aguarda A"]
        B3 --> B4["Retry / 409 Conflict"]
    end

    style A5 fill:#51cf66,color:#fff
    style B4 fill:#ff6b6b,color:#fff
```

## ğŸ› ï¸ Tecnologias Escolhidas

| Tecnologia         | VersÃ£o | Alternativas Consideradas | Por Que Escolhemos                                                                             |
| :----------------- | :----: | :------------------------ | :--------------------------------------------------------------------------------------------- |
| **PostgreSQL**     |   15   | MongoDB, MySQL            | Compliance ACID, Row-Level Locking, `SELECT FOR UPDATE`, suporte a arrays UUID                 |
| **Redis**          |   7    | Memcached, Zookeeper      | `SET NX PX` atÃ´mico para locks, TTL automÃ¡tico, latÃªncia < 1ms, cache de idempotÃªncia          |
| **Apache Kafka**   |  7.5   | RabbitMQ, AWS SQS         | Durabilidade de mensagens, replay de eventos, auditoria completa, alto throughput              |
| **NestJS**         |   11   | Express puro, Fastify     | InjeÃ§Ã£o de dependÃªncia nativa, TypeScript-first, arquitetura modular, Swagger integrado        |
| **TypeORM**        |  0.3   | Prisma, Sequelize         | Decorators nativos para entidades, `QueryRunner` para transaÃ§Ãµes, integraÃ§Ã£o direta com NestJS |
| **Docker Compose** |   â€”    | Kubernetes, bare-metal    | Setup simplificado, reprodutibilidade, orquestraÃ§Ã£o local de todos os serviÃ§os                 |

---

## ğŸš€ Como Executar

Existem **duas formas** de rodar o projeto:

| MÃ©todo                      | Ideal Para                     | Precisa criar `.env`? |   InstÃ¢ncias da API    |
| :-------------------------- | :----------------------------- | :-------------------: | :--------------------: |
| **Via Docker**              | Simular produÃ§Ã£o, testes E2E   |          âŒ           | 2 (portas 3000 e 3001) |
| **Via `npm run start:dev`** | Desenvolvimento com hot-reload |      âœ… **Sim**       |     1 (porta 3000)     |

### PrÃ©-requisitos

| Ferramenta     | VersÃ£o MÃ­nima         | NecessÃ¡rio Para       |
| :------------- | :-------------------- | :-------------------- |
| Git            | 2+                    | Ambos                 |
| Docker         | 20+                   | Docker                |
| Docker Compose | 2+                    | Docker                |
| Node.js        | 18+ (recomendado 20+) | Desenvolvimento local |
| npm            | 9+                    | Desenvolvimento local |

> **ğŸ“ Sobre o arquivo `.env`:**
>
> - **Via Docker:** NÃ£o precisa criar `.env` â€” as variÃ¡veis estÃ£o no `docker-compose.yml`
> - **Via `npm run start:dev`:** Ã‰ **obrigatÃ³rio** criar o arquivo `.env` na raiz do projeto (veja [Passo 4 da seÃ§Ã£o 5.2](#52-via-npm-run-startdev-desenvolvimento-local))

---

### 5.1 Via Docker (ProduÃ§Ã£o)

Esta opÃ§Ã£o sobe **toda a infraestrutura** (PostgreSQL, Redis, Kafka, Zookeeper) e **2 instÃ¢ncias da API** (portas 3000 e 3001) com um Ãºnico comando.

**Passo 1 â€” Clonar o repositÃ³rio:**

```bash
git clone https://github.com/daviixs/starsoft-backend-challenge.git
cd starsoft-backend-challenge
```

**Passo 2 â€” Subir todos os serviÃ§os:**

```bash
docker-compose up --build -d
```

> â± A primeira build pode levar ~2 minutos. O Docker Compose aguarda os health checks de PostgreSQL, Redis e Kafka antes de iniciar as instÃ¢ncias da API.

> âš ï¸ **NÃ£o Ã© necessÃ¡rio criar arquivo `.env`** ao rodar via Docker â€” todas as variÃ¡veis de ambiente estÃ£o configuradas no `docker-compose.yml`.

**Passo 3 â€” Verificar que tudo estÃ¡ rodando:**

```bash
# Ver status dos containers
docker-compose ps

# Health check completo (DB + Redis)
curl http://localhost:3000/api/health
```

**Resposta esperada:**

```json
{
  "status": "ok",
  "info": {
    "database": { "status": "up" },
    "redis": { "status": "up" }
  }
}
```

**Passo 4 â€” Popular dados iniciais:**

```bash
bash scripts/seed-data.sh
```

**Passo 5 â€” Acessar a API:**

| Recurso           | URL                                |
| :---------------- | :--------------------------------- |
| API (InstÃ¢ncia 1) | `http://localhost:3000/api`        |
| API (InstÃ¢ncia 2) | `http://localhost:3001/api`        |
| Swagger Docs      | `http://localhost:3000/api/docs`   |
| Health Check      | `http://localhost:3000/api/health` |

**Parar tudo:**

```bash
docker-compose down       # Para os containers (mantÃ©m dados)
docker-compose down -v    # Para os containers e apaga volumes (reset completo)
```

---

### 5.2 Via `npm run start:dev` (Desenvolvimento Local)

Esta opÃ§Ã£o roda a API localmente com **hot-reload** (reinicia automaticamente a cada alteraÃ§Ã£o de cÃ³digo). VocÃª ainda precisa de PostgreSQL, Redis e Kafka rodando â€” o jeito mais fÃ¡cil Ã© subi-los via Docker.

**Passo 1 â€” Clonar o repositÃ³rio:**

```bash
git clone https://github.com/daviixs/starsoft-backend-challenge.git
cd starsoft-backend-challenge
```

**Passo 2 â€” Instalar dependÃªncias:**

```bash
npm install
```

**Passo 3 â€” Subir apenas a infraestrutura (banco, cache, mensageria):**

```bash
docker-compose up -d postgres redis zookeeper kafka
```

> Isso sobe **apenas** PostgreSQL, Redis, Zookeeper e Kafka. A API **nÃ£o** sobe no Docker â€” vocÃª vai rodÃ¡-la localmente.

**Passo 4 â€” Criar arquivo `.env` (obrigatÃ³rio para dev local):**

> âš ï¸ **Importante:** Ao rodar localmente com `npm run start:dev`, vocÃª **PRECISA** criar o arquivo `.env` na raiz do projeto.

Crie um arquivo chamado `.env` na raiz (mesmo nÃ­vel do `package.json`):

```bash
NODE_ENV=development
PORT=3000
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=cinema
DATABASE_PASSWORD=cinema123
DATABASE_NAME=cinema_booking
REDIS_HOST=localhost
REDIS_PORT=6379
KAFKA_BROKERS=localhost:9092
KAFKA_CLIENT_ID=cinema-app-dev
KAFKA_GROUP_ID=cinema-booking-group
```

Ou copie o arquivo de exemplo (se existir):

```bash
cp .env.example .env
```

**Passo 5 â€” Iniciar a API em modo desenvolvimento:**

```bash
npm run start:dev
```

VocÃª verÃ¡ no terminal:

```
Application is running on: http://localhost:3000/api
Swagger docs: http://localhost:3000/api/docs
Health check: http://localhost:3000/api/health
```

> A cada alteraÃ§Ã£o em qualquer arquivo `.ts` dentro de `src/`, o NestJS recompila e reinicia automaticamente.

**Passo 6 â€” Verificar saÃºde e popular dados:**

```bash
curl http://localhost:3000/api/health
bash scripts/seed-data.sh
```

| Recurso      | URL                                |
| :----------- | :--------------------------------- |
| API          | `http://localhost:3000/api`        |
| Swagger Docs | `http://localhost:3000/api/docs`   |
| Health Check | `http://localhost:3000/api/health` |

---

## ğŸ“– DocumentaÃ§Ã£o da API (Swagger/OpenAPI)

A API possui documentaÃ§Ã£o interativa gerada automaticamente pelo **Swagger/OpenAPI**, acessÃ­vel em `http://localhost:3000/api/docs`. O Swagger Ã© configurado diretamente no `main.ts` utilizando o `@nestjs/swagger`, onde definimos tÃ­tulo, descriÃ§Ã£o e tags que organizam os endpoints por domÃ­nio (**Sessions** e **Bookings**). Cada rota da API Ã© decorada com `@ApiOperation`, `@ApiResponse` e `@ApiParam`, o que garante que a documentaÃ§Ã£o reflita exatamente o comportamento real do cÃ³digo â€” incluindo os cÃ³digos HTTP possÃ­veis (201, 404, 409, 410, 429) e a descriÃ§Ã£o de cada cenÃ¡rio. Os DTOs utilizam decorators do `class-validator` (como `@IsUUID`, `@IsArray`, `@Matches`) que sÃ£o automaticamente expostos no schema do Swagger, permitindo que qualquer desenvolvedor visualize as validaÃ§Ãµes exigidas por cada campo sem precisar ler o cÃ³digo-fonte. AlÃ©m de servir como referÃªncia para o time, o Swagger funciona como ferramenta de teste: Ã© possÃ­vel executar requisiÃ§Ãµes diretamente pela interface web, facilitando a depuraÃ§Ã£o durante o desenvolvimento.

```
http://localhost:3000/api/docs
```

---

## ğŸ“¡ Endpoints da API

Todos os endpoints utilizam o prefixo `/api`.

### ğŸ“Œ POST `/api/sessions` â€” Criar SessÃ£o de Cinema

Cria uma sessÃ£o com assentos gerados automaticamente (formato A1, A2, ..., B1, B2, ...).

**Request:**

```bash
curl -X POST http://localhost:3000/api/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "movieName": "Oppenheimer",
    "roomNumber": 1,
    "startsAt": "2026-02-15T19:00:00Z",
    "priceCents": 2500,
    "totalSeats": 24
  }'
```

| Campo        | Tipo     | Regras      | DescriÃ§Ã£o                           |
| :----------- | :------- | :---------- | :---------------------------------- |
| `movieName`  | string   | 1â€“255 chars | Nome do filme                       |
| `roomNumber` | int      | 1â€“20        | NÃºmero da sala                      |
| `startsAt`   | ISO 8601 | futuro      | Data/hora da sessÃ£o                 |
| `priceCents` | int      | â‰¥ 1000      | PreÃ§o em centavos (2500 = R$ 25,00) |
| `totalSeats` | int      | 16â€“100      | Quantidade de assentos              |

**Response `201`:**

```json
{
  "id": "7527e3a4-e170-4ac5-97a2-061fd1fe421e",
  "movieName": "Oppenheimer",
  "roomNumber": 1,
  "startsAt": "2026-02-15T19:00:00.000Z",
  "priceCents": 2500,
  "createdAt": "2026-02-09T21:50:00.000Z"
}
```

---

### ğŸ“Œ GET `/api/sessions` â€” Listar SessÃµes

```bash
curl http://localhost:3000/api/sessions
```

**Response `200`:**

```json
[
  {
    "id": "7527e3a4-e170-4ac5-97a2-061fd1fe421e",
    "movieName": "Oppenheimer",
    "roomNumber": 1,
    "startsAt": "2026-02-15T19:00:00.000Z",
    "priceCents": 2500,
    "createdAt": "2026-02-09T21:50:00.000Z"
  }
]
```

---

### ğŸ“Œ GET `/api/sessions/:id/availability` â€” Disponibilidade em Tempo Real

Retorna o estado de todos os assentos: disponÃ­veis, reservados e vendidos.

```bash
curl http://localhost:3000/api/sessions/7527e3a4-e170-4ac5-97a2-061fd1fe421e/availability
```

**Response `200`:**

```json
{
  "session": {
    "id": "7527e3a4-...",
    "movieName": "Oppenheimer",
    "roomNumber": 1,
    "startsAt": "2026-02-15T19:00:00.000Z",
    "priceCents": 2500
  },
  "availableSeats": ["A1", "A2", "A3", "B1", "B2", "B3"],
  "reservedSeats": ["C1"],
  "soldSeats": ["D1", "D2"]
}
```

| Status | DescriÃ§Ã£o                                 |
| :----- | :---------------------------------------- |
| `404`  | `{ "message": "Session {id} not found" }` |

---

### ğŸ“Œ POST `/api/bookings/reserve` â€” Reservar Assentos

Reserva assentos com **lock distribuÃ­do** + **transaÃ§Ã£o PostgreSQL**. A reserva expira em **30 segundos** se o pagamento nÃ£o for confirmado.

**Request:**

```bash
curl -X POST http://localhost:3000/api/bookings/reserve \
  -H "Content-Type: application/json" \
  -d '{
    "userId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "sessionId": "7527e3a4-e170-4ac5-97a2-061fd1fe421e",
    "seatNumbers": ["A1", "A2"]
  }'
```

| Campo         | Tipo     | Regras                             | DescriÃ§Ã£o          |
| :------------ | :------- | :--------------------------------- | :----------------- |
| `userId`      | UUID v4  | obrigatÃ³rio                        | ID do usuÃ¡rio      |
| `sessionId`   | UUID v4  | obrigatÃ³rio                        | ID da sessÃ£o       |
| `seatNumbers` | string[] | 1â€“10 itens, formato `[A-H]\d{1,2}` | Assentos desejados |

**Response `201`:**

```json
{
  "id": "res-uuid-...",
  "userId": "a1b2c3d4-...",
  "sessionId": "7527e3a4-...",
  "seatIds": ["seat-uuid-1", "seat-uuid-2"],
  "expiresAt": "2026-02-09T22:30:30.000Z",
  "status": "pending",
  "message": "Reservation confirmed. Complete payment within 30 seconds."
}
```

| Status | DescriÃ§Ã£o                                          |
| :----- | :------------------------------------------------- |
| `409`  | Assento(s) indisponÃ­vel(is) â€” jÃ¡ reservado/vendido |
| `404`  | SessÃ£o nÃ£o encontrada                              |
| `429`  | Rate limit excedido (mÃ¡x 5 reservas/minuto por IP) |

---

### ğŸ“Œ POST `/api/bookings/confirm` â€” Confirmar Pagamento

Converte uma reserva pendente em venda definitiva. Os assentos passam de `reserved` para `sold`.

**Request:**

```bash
curl -X POST http://localhost:3000/api/bookings/confirm \
  -H "Content-Type: application/json" \
  -d '{
    "reservationId": "res-uuid-...",
    "userId": "a1b2c3d4-..."
  }'
```

| Campo            | Tipo    | Regras      | DescriÃ§Ã£o                           |
| :--------------- | :------ | :---------- | :---------------------------------- |
| `reservationId`  | UUID v4 | obrigatÃ³rio | ID da reserva                       |
| `userId`         | UUID v4 | obrigatÃ³rio | ID do usuÃ¡rio que fez a reserva     |
| `idempotencyKey` | string  | opcional    | Chave para evitar dupla confirmaÃ§Ã£o |

**Response `201`:**

```json
{
  "id": "sale-uuid-...",
  "reservationId": "res-uuid-...",
  "userId": "a1b2c3d4-...",
  "totalAmountCents": 5000,
  "confirmedAt": "2026-02-09T22:30:15.000Z",
  "message": "Payment confirmed successfully!"
}
```

| Status | DescriÃ§Ã£o                                 |
| :----- | :---------------------------------------- |
| `404`  | Reserva nÃ£o encontrada                    |
| `410`  | Reserva expirada (passou dos 30 segundos) |
| `409`  | ConfirmaÃ§Ã£o jÃ¡ em andamento (lock)        |

---

### ğŸ“Œ GET `/api/bookings/user/:userId` â€” HistÃ³rico de Compras

```bash
curl http://localhost:3000/api/bookings/user/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

**Response `200`:**

```json
{
  "userId": "a1b2c3d4-...",
  "totalPurchases": 1,
  "purchases": [
    {
      "id": "sale-uuid-...",
      "totalAmountCents": 5000,
      "confirmedAt": "2026-02-09T22:30:15.000Z",
      "movie": "Oppenheimer",
      "startsAt": "2026-02-15T19:00:00.000Z",
      "seats": 2
    }
  ]
}
```

---

### ğŸ“Œ GET `/api/bookings/reservation/:id` â€” Status de Reserva

```bash
curl http://localhost:3000/api/bookings/reservation/res-uuid-...
```

**Response `200`:**

```json
{
  "id": "res-uuid-...",
  "userId": "a1b2c3d4-...",
  "sessionId": "7527e3a4-...",
  "seatIds": ["seat-uuid-1", "seat-uuid-2"],
  "expiresAt": "2026-02-09T22:30:30.000Z",
  "status": "pending",
  "isExpired": false,
  "timeRemaining": 22
}
```

---

### ğŸ“Œ GET `/api/health` â€” Health Check

```bash
curl http://localhost:3000/api/health
```

**Response `200`:**

```json
{
  "status": "ok",
  "info": {
    "database": { "status": "up" },
    "redis": { "status": "up" }
  }
}
```

---

## ğŸ” EstratÃ©gias de ConcorrÃªncia

O sistema implementa **3 camadas de proteÃ§Ã£o** para garantir integridade em um ambiente distribuÃ­do com mÃºltiplas instÃ¢ncias:

### 7.1 Camada 1 â€” Redis Distributed Lock (`SET NX PX`)

A primeira barreira Ã© um **lock distribuÃ­do via Redis**. Antes de qualquer operaÃ§Ã£o no banco, a API tenta adquirir locks exclusivos para cada assento:

```
SET seat:lock:{sessionId}:{seatNumber} {lockValue} PX 5000 NX
```

- **`NX`** â€” sÃ³ seta se a chave **nÃ£o existir** (exclusÃ£o mÃºtua)
- **`PX 5000`** â€” expira automaticamente em 5s (previne locks Ã³rfÃ£os)
- **LiberaÃ§Ã£o via Lua Script** â€” garante que apenas o dono do lock pode liberÃ¡-lo

```lua
if redis.call("get", KEYS[1]) == ARGV[1] then
  return redis.call("del", KEYS[1])
else
  return 0
end
```

**Por que Redis e nÃ£o sÃ³ PostgreSQL?** Redis opera em < 1ms. Ele rejeita requisiÃ§Ãµes duplicadas **antes** de abrir uma transaÃ§Ã£o no banco, economizando conexÃµes do pool.

---

### 7.2 Camada 2 â€” PostgreSQL Transaction + `SELECT FOR UPDATE`

Mesmo com o Redis protegendo, a transaÃ§Ã£o no PostgreSQL garante **ACID**:

```sql
BEGIN;

  SELECT * FROM seats
  WHERE session_id = $1 AND seat_number IN ($2, $3)
    AND status = 'available'
  ORDER BY seat_number ASC
  FOR UPDATE;  -- Lock pessimista na linha

  UPDATE seats SET status = 'reserved', reserved_until = $4
  WHERE id IN (...);

  INSERT INTO reservations (user_id, session_id, seat_ids, expires_at, status)
  VALUES ($5, $1, $6, $4, 'pending');

COMMIT;
```

O `FOR UPDATE` bloqueia as linhas dos assentos atÃ© o `COMMIT`, impedindo que outra transaÃ§Ã£o leia/modifique os mesmos dados.

---

### 7.3 Camada 3 â€” IdempotÃªncia com Cache

RequisiÃ§Ãµes idÃªnticas (mesmo usuÃ¡rio + mesma sessÃ£o + mesmos assentos) dentro de **30 segundos** retornam o resultado cacheado:

```
Chave: reserve:{userId}:{sessionId}:{seatNumbers sorted}
TTL:   30 segundos
```

Isso protege contra:

- Clique duplo do usuÃ¡rio
- Retry automÃ¡tico do frontend
- Timeout + reenvio

---

### Diagrama de SequÃªncia â€” Fluxo de Reserva

```mermaid
sequenceDiagram
    participant C as Cliente
    participant A as API NestJS
    participant R as Redis
    participant P as PostgreSQL
    participant K as Kafka

    C->>A: POST /bookings/reserve
    A->>R: GET idempotency cache
    R-->>A: null - nao cacheado

    A->>R: SET NX seat lock A1
    R-->>A: OK - lock adquirido
    A->>R: SET NX seat lock A2
    R-->>A: OK - lock adquirido

    A->>P: BEGIN TRANSACTION
    A->>P: SELECT ... FOR UPDATE seats A1, A2
    P-->>A: seats disponiveis
    A->>P: UPDATE seats SET status=reserved
    A->>P: INSERT INTO reservations
    A->>P: COMMIT

    A->>K: publish booking.reserved
    A->>R: SETEX idempotency cache 30s

    A->>R: DEL seat lock A1
    A->>R: DEL seat lock A2

    A-->>C: 201 Created - reserva pendente
```

---

### 7.4 Como Garantimos IdempotÃªncia

| OperaÃ§Ã£o    | Chave de IdempotÃªncia                         | TTL  |
| :---------- | :-------------------------------------------- | :--- |
| Reserva     | `reserve:{userId}:{sessionId}:{seats sorted}` | 30s  |
| ConfirmaÃ§Ã£o | `confirm:{reservationId}:{userId}`            | 300s |

AlÃ©m disso, a tabela `sales` possui constraint `UNIQUE(reservation_id)`, impedindo dupla confirmaÃ§Ã£o mesmo em caso de falha do cache.

---

## ğŸ 9. SoluÃ§Ã£o Detalhada para Problemas de ConcorrÃªncia

Esta seÃ§Ã£o explica **em profundidade** como o sistema lida com os trÃªs maiores desafios de um sistema distribuÃ­do de reservas.

---

### 9.1 SoluÃ§Ã£o para Race Conditions (CondiÃ§Ã£o de Corrida)

#### O Problema

Quando 10 usuÃ¡rios clicam em "Reservar" no **mesmo assento ao mesmo tempo**, sem proteÃ§Ã£o adequada, todos leem o assento como `available` e todos executam o `UPDATE` â€” resultando em **10 reservas para 1 Ãºnico assento**.

```
Timeline SEM proteÃ§Ã£o:

  t=0ms  UsuÃ¡rio A â†’ SELECT status FROM seats WHERE A1  â†’ 'available' âœ…
  t=1ms  UsuÃ¡rio B â†’ SELECT status FROM seats WHERE A1  â†’ 'available' âœ…  â† STALE READ!
  t=2ms  UsuÃ¡rio A â†’ UPDATE seats SET status='reserved'  â†’ OK
  t=3ms  UsuÃ¡rio B â†’ UPDATE seats SET status='reserved'  â†’ OK  â† DOUBLE BOOKING!
```

#### Nossa SoluÃ§Ã£o: Defesa em 3 Camadas

Implementamos uma estratÃ©gia de **defesa em profundidade** (defense-in-depth) onde cada camada complementa a anterior:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAMADA 1: Redis Distributed Lock (< 1ms)        â”‚
â”‚   Barreira rÃ¡pida â€” rejeita 9 de 10 requisiÃ§Ãµes         â”‚
â”‚   antes de tocar no banco de dados                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         CAMADA 2: PostgreSQL SELECT FOR UPDATE           â”‚
â”‚   Garantia ACID â€” mesmo se o Redis falhar,              â”‚
â”‚   o banco garante consistÃªncia via lock pessimista       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         CAMADA 3: Cache de IdempotÃªncia (Redis)          â”‚
â”‚   DeduplicaÃ§Ã£o â€” requisiÃ§Ãµes idÃªnticas em < 30s          â”‚
â”‚   retornam o mesmo resultado sem reprocessar             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Por que 3 camadas e nÃ£o apenas 1?

| CenÃ¡rio de falha                         |  Camada 1 (Redis Lock)   | Camada 2 (PG Transaction) | Camada 3 (IdempotÃªncia) |
| :--------------------------------------- | :----------------------: | :-----------------------: | :---------------------: |
| 10 usuÃ¡rios simultÃ¢neos no mesmo assento | âœ… 1 passa, 9 bloqueados |             â€”             |            â€”            |
| Redis cai momentaneamente                |         âŒ Falha         |  âœ… `FOR UPDATE` protege  |            â€”            |
| UsuÃ¡rio clica 2x rÃ¡pido                  |   âœ… Lock ainda ativo    |             â€”             |    âœ… Retorna cache     |
| Retry do frontend apÃ³s timeout           |            â€”             |             â€”             |    âœ… Retorna cache     |
| Lock Redis expira antes do COMMIT        |      âŒ Outro entra      | âœ… `FOR UPDATE` bloqueia  |            â€”            |

> **Escolha tÃ©cnica:** A Camada 1 (Redis) Ã© otimista e rÃ¡pida. A Camada 2 (PostgreSQL) Ã© pessimista e segura. Juntas, oferecem **velocidade + garantia ACID**.

#### Fluxo Detalhado com Race Condition

```mermaid
sequenceDiagram
    participant A as Usuario A
    participant B as Usuario B
    participant API as API NestJS
    participant R as Redis
    participant PG as PostgreSQL

    Note over A,B: Ambos clicam em Reservar Assento A1 simultaneamente

    A->>API: POST /reserve seat A1
    B->>API: POST /reserve seat A1

    API->>R: SET seat lock A1 val_A PX 5000 NX
    R-->>API: OK - A obteve o lock

    API->>R: SET seat lock A1 val_B PX 5000 NX
    R-->>API: NULL - B bloqueado

    Note over B,API: B recebe 409 Conflict IMEDIATAMENTE sem tocar no PostgreSQL

    API-->>B: 409 Seat unavailable

    API->>PG: BEGIN + SELECT FOR UPDATE WHERE seat=A1
    PG-->>API: seat A1 status=available
    API->>PG: UPDATE reserved + INSERT reservation + COMMIT

    API->>R: DEL seat lock A1
    API-->>A: 201 Created - reserva pendente
```

#### CÃ³digo que implementa a proteÃ§Ã£o (BookingsService)

```typescript
async reserveSeats(dto: ReserveSeatsDto): Promise<Reservation> {
  // CAMADA 3: IdempotÃªncia â€” retorna cache se existir
  const cached = await this.redisLock.getCache<Reservation>(idempotencyKey);
  if (cached) return cached;

  // CAMADA 1: Redis Lock â€” aquisiÃ§Ã£o em ORDEM ALFABÃ‰TICA
  const locks = await this.redisLock.acquireMultiple(lockKeys, 5000);
  if (!locks) throw new SeatUnavailableException(seatNumbers);

  try {
    // CAMADA 2: PostgreSQL â€” SELECT FOR UPDATE (lock pessimista)
    const seats = await queryRunner.manager.find(Seat, {
      where: { sessionId, seatNumber: In(sortedSeatNumbers), status: 'available' },
      lock: { mode: 'pessimistic_write' }, // â† FOR UPDATE
    });

    if (seats.length !== sortedSeatNumbers.length) {
      throw new SeatUnavailableException(unavailable);
    }
    // ... UPDATE + INSERT + COMMIT
  } finally {
    await this.redisLock.releaseMultiple(locks); // Sempre libera
  }
}
```

---

### 9.2 CoordenaÃ§Ã£o entre InstÃ¢ncias

#### O Problema

Com **2+ instÃ¢ncias** da API rodando simultaneamente (cinema-app-1 na porta 3000, cinema-app-2 na porta 3001), variÃ¡veis locais e locks em memÃ³ria **nÃ£o servem** â€” cada instÃ¢ncia vive em um processo isolado.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    cinema-app-1      â”‚    â”‚    cinema-app-2      â”‚
â”‚  lock em memÃ³ria: {} â”‚    â”‚  lock em memÃ³ria: {} â”‚
â”‚  (isolado!)          â”‚    â”‚  (isolado!)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚  Ambos acham que o seat A1 estÃ¡ livre!
           â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          PostgreSQL (compartilhado)      â”‚
    â”‚  Sem coordenaÃ§Ã£o â†’ DOUBLE BOOKING        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Nossa SoluÃ§Ã£o: Redis como Ponto Central de CoordenaÃ§Ã£o

O Redis Ã© o **Ãºnico ponto de verdade** para locks, acessÃ­vel por todas as instÃ¢ncias:

```mermaid
flowchart TD
    subgraph inst1["Instance 1 :3000"]
        A1["POST /reserve seat A1"] --> R1["Redis: SET NX seat lock A1"]
    end

    subgraph inst2["Instance 2 :3001"]
        A2["POST /reserve seat A1"] --> R2["Redis: SET NX seat lock A1"]
    end

    R1 --> Redis[("Redis - compartilhado")]
    R2 --> Redis

    Redis -->|"OK primeiro"| S1["Inst 1 prossegue"]
    Redis -->|"NULL segundo"| S2["Inst 2 bloqueada"]

    S1 --> PG[("PostgreSQL")]
    S2 --> E["409 Conflict"]

    style S1 fill:#51cf66,color:#fff
    style E fill:#ff6b6b,color:#fff
```

#### Mecanismos de CoordenaÃ§Ã£o Implementados

| Mecanismo                 | Tecnologia              | PropÃ³sito                                   | Escopo          |
| :------------------------ | :---------------------- | :------------------------------------------ | :-------------- |
| **Lock DistribuÃ­do**      | Redis `SET NX PX`       | ExclusÃ£o mÃºtua entre instÃ¢ncias             | Por assento     |
| **Lock Pessimista**       | PostgreSQL `FOR UPDATE` | ConsistÃªncia ACID final                     | Por linha no DB |
| **Cache de IdempotÃªncia** | Redis `SETEX`           | Evitar reprocessamento duplicado            | Por requisiÃ§Ã£o  |
| **Mensageria**            | Kafka topics            | Notificar todas as instÃ¢ncias sobre eventos | Global          |
| **Cron DistribuÃ­do**      | `@nestjs/schedule`      | ExpiraÃ§Ã£o de reservas (cada instÃ¢ncia roda) | Por instÃ¢ncia   |

#### Por que Redis SET NX funciona entre instÃ¢ncias?

O comando `SET key value PX ttl NX` Ã© **atÃ´mico no Redis** â€” o servidor Redis processa um comando por vez (single-threaded para operaÃ§Ãµes de dados). Quando duas instÃ¢ncias enviam `SET NX` para a mesma chave **no mesmo microssegundo**:

```
t=0.000ms  Inst 1 â†’ SET seat:lock:A1 "inst1-lock" PX 5000 NX
t=0.000ms  Inst 2 â†’ SET seat:lock:A1 "inst2-lock" PX 5000 NX

â†’ Redis serializa internamente:
  1. Processa Inst 1 â†’ chave nÃ£o existe â†’ SET â†’ retorna OK
  2. Processa Inst 2 â†’ chave JÃ existe â†’ retorna NULL

â†’ Resultado: garantia de que APENAS 1 instÃ¢ncia obtÃ©m o lock
```

#### LiberaÃ§Ã£o segura com Lua Script

O Lua script garante que **apenas o dono do lock** pode liberÃ¡-lo â€” evitando que uma instÃ¢ncia libere o lock de outra:

```lua
-- Executado atomicamente no Redis
if redis.call("get", KEYS[1]) == ARGV[1] then
  return redis.call("del", KEYS[1])   -- SÃ³ deleta se o valor bate
else
  return 0                             -- NÃ£o Ã© meu lock, nÃ£o toco
end
```

**CenÃ¡rio que isso protege:**

```
t=0ms    Inst 1 adquire lock (valor="inst1-abc")
t=4999ms Lock estÃ¡ prestes a expirar
t=5000ms Lock expira automaticamente (PX 5000)
t=5001ms Inst 2 adquire o MESMO lock (valor="inst2-xyz")
t=5002ms Inst 1 termina processamento e tenta DEL
         â†’ Lua script verifica: "inst1-abc" â‰  "inst2-xyz"
         â†’ NÃƒO deleta! Lock da Inst 2 permanece seguro âœ…
```

#### Kafka como canal de comunicaÃ§Ã£o assÃ­ncrona

O Kafka permite que eventos de uma instÃ¢ncia sejam **consumidos por todas as outras**:

```mermaid
flowchart LR
    subgraph inst1["Instancia 1"]
        A1["Reserva criada"] --> P1["Kafka Producer"]
    end

    P1 --> T["Topic: booking.reserved"]

    T --> C1["Instancia 1: Expiry Worker"]
    T --> C2["Instancia 2: Expiry Worker"]

    C1 --> S1["setTimeout expire, 30s"]
    C2 --> S2["setTimeout expire, 30s"]

    S1 --> CHECK{"Reserva ainda pendente?"}
    S2 --> CHECK
    CHECK -- Sim --> EXP["Expira reserva"]
    CHECK -- Nao --> SKIP["Ignora - ja confirmada"]
```

> **Nota:** Mesmo que ambas as instÃ¢ncias tentem expirar a mesma reserva, a operaÃ§Ã£o Ã© idempotente â€” a transaÃ§Ã£o no PostgreSQL verifica `status = 'pending'` antes de atualizar.

---

### 9.3 PrevenÃ§Ã£o de Deadlocks

#### O Problema

Um **deadlock** ocorre quando dois processos ficam travados esperando um pelo outro, indefinidamente:

```
SEM ordenaÃ§Ã£o de locks:

  Inst 1: adquire lock A1 â†’ tenta lock B2 â†’ â³ esperando Inst 2 liberar B2
  Inst 2: adquire lock B2 â†’ tenta lock A1 â†’ â³ esperando Inst 1 liberar A1

  â†’ DEADLOCK! Nenhum dos dois avanÃ§a. Sistema trava.
```

#### Nossa SoluÃ§Ã£o: Ordered Lock Acquisition (AquisiÃ§Ã£o Ordenada de Locks)

A tÃ©cnica clÃ¡ssica para prevenir deadlocks Ã© **impor uma ordem global** na aquisiÃ§Ã£o de locks. Se todos os processos adquirem locks **na mesma ordem**, Ã© impossÃ­vel formarem um ciclo de espera.

**ImplementaÃ§Ã£o:**

```typescript
// RedisLockService.acquireMultiple()
async acquireMultiple(keys: string[], ttlMs = 5000) {
  const sortedKeys = [...keys].sort(); // â† CHAVE: ordem alfabÃ©tica
  const acquiredLocks = new Map<string, string>();

  for (const key of sortedKeys) {
    const lockValue = await this.acquire(key, ttlMs);
    if (!lockValue) {
      // Se falhar em QUALQUER lock, libera TODOS os jÃ¡ adquiridos
      await this.releaseMultiple(acquiredLocks);
      return null;
    }
    acquiredLocks.set(key, lockValue);
  }
  return acquiredLocks;
}
```

#### Prova de que NÃƒO hÃ¡ deadlock

```mermaid
flowchart TD
    subgraph sem["Sem Ordenacao - DEADLOCK"]
        D1["Inst 1: lock B2 OK"] --> D2["Inst 1: lock A1 espera"]
        D3["Inst 2: lock A1 OK"] --> D4["Inst 2: lock B2 espera"]
        D2 -. "espera" .-> D3
        D4 -. "espera" .-> D1
    end

    subgraph com["Com Ordenacao - SEGURO"]
        S1["Inst 1: sort B2,A1 para A1,B2"] --> S2["Inst 1: lock A1 OK"]
        S2 --> S3["Inst 1: lock B2 OK"]
        S4["Inst 2: sort A1,B2 para A1,B2"] --> S5["Inst 2: lock A1 aguarda"]
        S5 --> S6["Inst 2: retry/timeout 409"]
    end

    style D2 fill:#ff6b6b,color:#fff
    style D4 fill:#ff6b6b,color:#fff
    style S3 fill:#51cf66,color:#fff
    style S6 fill:#ffd43b,color:#000
```

#### Mecanismo de Rollback em Falha Parcial

Se a aquisiÃ§Ã£o do **segundo lock** falha (por timeout ou lock jÃ¡ adquirido por outro), o sistema **libera todos os locks jÃ¡ adquiridos** antes de retornar erro:

```
CenÃ¡rio: UsuÃ¡rio quer reservar [A1, B2, C3]

  1. sort() â†’ [A1, B2, C3]
  2. lock(A1) â†’ OK âœ…
  3. lock(B2) â†’ OK âœ…
  4. lock(C3) â†’ FALHA âŒ (outro usuÃ¡rio jÃ¡ tem)

  â†’ Rollback automÃ¡tico:
    - release(A1) âœ…
    - release(B2) âœ…
    - Retorna null â†’ Controller lanÃ§a 409 Conflict

  â†’ Nenhum lock fica "Ã³rfÃ£o" ocupando recursos
```

#### ProteÃ§Ã£o adicional: TTL nos locks

Mesmo em cenÃ¡rios extremos (crash da aplicaÃ§Ã£o, network partition), os locks **expiram automaticamente** apÃ³s 5 segundos (`PX 5000`):

```
t=0s     Inst 1 adquire lock(A1) com TTL=5s
t=0.5s   Inst 1 CRASH! (OOM, kill -9, etc)
t=5s     Lock expira automaticamente no Redis
t=5.1s   Inst 2 consegue adquirir lock(A1) normalmente

â†’ Sistema se recupera sozinho, sem intervenÃ§Ã£o manual
```

#### Tabela resumo: todas as proteÃ§Ãµes contra deadlocks

| ProteÃ§Ã£o                      | Mecanismo                                         | Onde                                 |
| :---------------------------- | :------------------------------------------------ | :----------------------------------- |
| **OrdenaÃ§Ã£o de locks**        | `sort()` antes de `acquireMultiple()`             | `RedisLockService`                   |
| **Rollback em falha parcial** | Libera todos os locks adquiridos se um falhar     | `RedisLockService.acquireMultiple()` |
| **TTL automÃ¡tico**            | `PX 5000` em todo `SET NX`                        | Redis                                |
| **LiberaÃ§Ã£o via Lua**         | Script atÃ´mico que verifica ownership             | `RedisLockService.release()`         |
| **Retry com backoff**         | Tentativas com `delay * (attempt + 1)`            | `RedisLockService.acquire()`         |
| **OrdenaÃ§Ã£o no SQL**          | `ORDER BY seat_number ASC` no `SELECT FOR UPDATE` | `BookingsService`                    |

---

## ğŸ— Arquitetura Detalhada

### 8.1 Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Compose                           â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   cinema-app-1     â”‚        â”‚   cinema-app-2     â”‚          â”‚
â”‚  â”‚   (NestJS :3000)   â”‚        â”‚   (NestJS :3001)   â”‚          â”‚
â”‚  â”‚                    â”‚        â”‚                    â”‚          â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚
â”‚  â”‚ â”‚ Controllers  â”‚   â”‚        â”‚ â”‚ Controllers  â”‚   â”‚          â”‚
â”‚  â”‚ â”‚ Services     â”‚   â”‚        â”‚ â”‚ Services     â”‚   â”‚          â”‚
â”‚  â”‚ â”‚ Workers      â”‚   â”‚        â”‚ â”‚ Workers      â”‚   â”‚          â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                             â”‚                     â”‚
â”‚       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                â”‚
â”‚       â”‚                                       â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                â”‚
â”‚  â”‚PostgreSQLâ”‚   â”‚  Redis  â”‚   â”‚    Kafka     â”‚â”‚                â”‚
â”‚  â”‚  :5432  â”‚   â”‚  :6379  â”‚   â”‚    :9092     â”‚â”‚                â”‚
â”‚  â”‚         â”‚   â”‚         â”‚   â”‚              â”‚â”‚                â”‚
â”‚  â”‚â€¢ SessÃµesâ”‚   â”‚â€¢ Locks  â”‚   â”‚â€¢ booking.*   â”‚â”‚                â”‚
â”‚  â”‚â€¢ Seats  â”‚   â”‚â€¢ Cache  â”‚   â”‚  .reserved   â”‚â”‚                â”‚
â”‚  â”‚â€¢ Reserv.â”‚   â”‚â€¢ Idemp. â”‚   â”‚  .confirmed  â”‚â”‚                â”‚
â”‚  â”‚â€¢ Sales  â”‚   â”‚         â”‚   â”‚  .expired    â”‚â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                â”‚
â”‚                                               â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 Fluxo de Reserva (Step by Step)

1. **Cliente** â†’ `POST /api/bookings/reserve` com `{ userId, sessionId, seatNumbers }`
2. **API** â†’ Verifica se a sessÃ£o existe no PostgreSQL
3. **API** â†’ Verifica cache de idempotÃªncia no Redis
4. **API** â†’ `Redis.SET NX PX` para cada assento (ordem alfabÃ©tica)
5. **API** â†’ `PostgreSQL BEGIN TRANSACTION`
6. **API** â†’ `SELECT FOR UPDATE` nos assentos
7. **API** â†’ `UPDATE seats SET status = 'reserved'`
8. **API** â†’ `INSERT INTO reservations`
9. **API** â†’ `COMMIT`
10. **API** â†’ `Kafka.publish('booking.reserved', { ... })`
11. **API** â†’ `Redis.SETEX` cache de idempotÃªncia (30s)
12. **API** â†’ Libera locks no Redis via Lua script
13. **API** â†’ Retorna `201 { reservationId, expiresAt }`

### 8.3 Worker de ExpiraÃ§Ã£o de Reservas

O `ReservationExpiryWorker` opera em **duas frentes**:

**1. Cron Job (a cada 10 segundos):**

- Busca reservas com `status = 'pending'` e `expires_at < NOW()`
- Para cada reserva expirada:
  - `UPDATE seats SET status = 'available'`
  - `UPDATE reservations SET status = 'expired'`
  - `Kafka.publish('booking.expired', { ... })`

**2. Kafka Consumer (event-driven):**

- Escuta o tÃ³pico `booking.reserved`
- Ao receber evento, agenda `setTimeout` para verificar expiraÃ§Ã£o apÃ³s o TTL
- Garante expiraÃ§Ã£o pontual mesmo entre ciclos do cron

### 8.4 TÃ³picos Kafka

| TÃ³pico              | Evento               | Publicado Quando                 |
| :------------------ | :------------------- | :------------------------------- |
| `booking.reserved`  | Reserva criada       | ApÃ³s commit da reserva           |
| `booking.confirmed` | Pagamento confirmado | ApÃ³s commit da venda             |
| `booking.expired`   | Reserva expirada     | ApÃ³s expirar reserva pelo worker |

---

## ğŸ“ Estrutura do Projeto

```
src/
â”œâ”€â”€ main.ts                          # Bootstrap, Swagger, ValidationPipe
â”œâ”€â”€ app.module.ts                    # MÃ³dulo raiz (Config, TypeORM, Throttler)
â”œâ”€â”€ app.controller.ts                # Controller raiz
â”œâ”€â”€ app.service.ts                   # Service raiz
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ database.config.ts           # ConfiguraÃ§Ã£o PostgreSQL (TypeORM)
â”‚   â”œâ”€â”€ redis.config.ts              # ConfiguraÃ§Ã£o Redis (ioredis)
â”‚   â””â”€â”€ kafka.config.ts              # ConfiguraÃ§Ã£o Kafka (kafkajs)
â”‚
â”œâ”€â”€ sessions/
â”‚   â”œâ”€â”€ sessions.module.ts           # MÃ³dulo de sessÃµes
â”‚   â”œâ”€â”€ sessions.controller.ts       # POST /sessions, GET /sessions, GET /sessions/:id/availability
â”‚   â””â”€â”€ sessions.service.ts          # LÃ³gica de criaÃ§Ã£o e consulta de sessÃµes
â”‚
â”œâ”€â”€ booking/
â”‚   â”œâ”€â”€ booking.module.ts            # MÃ³dulo de reservas
â”‚   â”œâ”€â”€ booking.controller.ts        # POST /bookings/reserve, POST /bookings/confirm, GET /bookings/user/:id
â”‚   â”œâ”€â”€ bookings.service.ts          # LÃ³gica de reserva com locks + transaÃ§Ãµes
â”‚   â””â”€â”€ use-cases/
â”‚       â””â”€â”€ reserve-seats.use-case.ts
â”‚
â”œâ”€â”€ modules/sessions/entities/
â”‚   â”œâ”€â”€ session.entity.ts            # Entidade Session (filme, sala, horÃ¡rio, preÃ§o)
â”‚   â”œâ”€â”€ seat.entity.ts               # Entidade Seat (assento, status, reservedUntil)
â”‚   â”œâ”€â”€ reservation.entity.ts        # Entidade Reservation (userId, seatIds[], expiresAt)
â”‚   â””â”€â”€ sale.entity.ts               # Entidade Sale (reservationId, totalAmountCents)
â”‚
â”œâ”€â”€ dto/
â”‚   â”œâ”€â”€ create-session.dto.ts        # DTO com validaÃ§Ã£o (class-validator)
â”‚   â”œâ”€â”€ reserve-seats.dto.ts         # DTO com regex para formato de assento
â”‚   â””â”€â”€ confirm-payment.dto.ts       # DTO com idempotencyKey opcional
â”‚
â”œâ”€â”€ redis/
â”‚   â””â”€â”€ redis-lock.service.ts        # Locks distribuÃ­dos (SET NX PX, Lua script, acquireMultiple)
â”‚
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ kafka.module.ts              # MÃ³dulo Kafka
â”‚   â”œâ”€â”€ kafka.producer.service.ts    # PublicaÃ§Ã£o de eventos
â”‚   â””â”€â”€ kafka-consumer.service.ts    # Consumo de eventos
â”‚
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ reservation-expiry.worker.ts # Cron + Kafka consumer para expirar reservas
â”‚
â”œâ”€â”€ health/
â”‚   â”œâ”€â”€ health.module.ts             # MÃ³dulo de health check
â”‚   â””â”€â”€ health.controller.ts         # GET /health (DB + Redis)
â”‚
â”œâ”€â”€ exceptions/
â”‚   â””â”€â”€ business.exceptions.ts       # Exceptions customizadas (409, 404, 410)
â”‚
â””â”€â”€ shared/
    â””â”€â”€ shared.module.ts             # MÃ³dulo compartilhado (Redis, Kafka exports)

test/
â”œâ”€â”€ jest-e2e.json                    # ConfiguraÃ§Ã£o Jest E2E
â”œâ”€â”€ app.e2e-spec.ts                  # Testes E2E da aplicaÃ§Ã£o
â””â”€â”€ bookings.e2e-spec.ts             # Testes E2E de reservas e concorrÃªncia

scripts/
â”œâ”€â”€ seed-data.sh                     # Popular dados iniciais
â”œâ”€â”€ test-full-flow.sh                # Teste completo do fluxo (reserva â†’ pagamento)
â””â”€â”€ concurrency-test.sh              # Teste de concorrÃªncia (10 usuÃ¡rios simultÃ¢neos)
```

---

## ğŸ’¡ DecisÃµes TÃ©cnicas

### Por que Redis + PostgreSQL (HÃ­brido)?

| Componente     | Responsabilidade                                          | LatÃªncia |
| :------------- | :-------------------------------------------------------- | :------- |
| **Redis**      | CoordenaÃ§Ã£o rÃ¡pida entre instÃ¢ncias (locks, idempotÃªncia) | < 1ms    |
| **PostgreSQL** | PersistÃªncia ACID, garantia de consistÃªncia final         | ~5ms     |

Usar **apenas PostgreSQL** exigiria `SELECT FOR UPDATE` em todas as requisiÃ§Ãµes, sobrecarregando o pool de conexÃµes. O Redis filtra requisiÃ§Ãµes concorrentes **antes** de tocar no banco.

### Por que Kafka e nÃ£o RabbitMQ?

- **Durabilidade**: mensagens persistidas em disco com retenÃ§Ã£o configurÃ¡vel
- **Replay**: possibilidade de reprocessar eventos passados
- **Auditoria completa**: log imutÃ¡vel de todos os eventos (`reserved`, `confirmed`, `expired`)
- **Scaling**: partiÃ§Ãµes permitem consumo paralelo em mÃºltiplas instÃ¢ncias

### Por que TypeORM e nÃ£o Prisma?

- **QueryRunner**: controle total sobre transaÃ§Ãµes (`BEGIN`, `COMMIT`, `ROLLBACK`)
- **Lock pessimista**: suporte nativo a `SELECT FOR UPDATE` via `{ lock: { mode: 'pessimistic_write' } }`
- **Decorators**: entidades declarativas integradas ao ecossistema NestJS
- **Auto-sync**: `synchronize: true` para desenvolvimento rÃ¡pido

### Por que Rate Limiting com @nestjs/throttler?

- **60 req/min** por IP (default)
- **5 req/min** para rotas sensÃ­veis (reserve/confirm) â€” evita abuse de recursos
- ConfiguraÃ§Ã£o por route via decorator `@Throttle()`

---

## ğŸ›¡ï¸ Rate Limiting

O sistema implementa **Rate Limiting** (limitaÃ§Ã£o de taxa de requisiÃ§Ãµes) utilizando o mÃ³dulo `@nestjs/throttler`, que atua como uma camada de proteÃ§Ã£o contra abuso e ataques de forÃ§a bruta. O throttler Ã© registrado como **Guard global** no `AppModule`, o que significa que **todas as rotas da API** sÃ£o automaticamente protegidas sem necessidade de configuraÃ§Ã£o individual. Definimos dois profiles de throttling: o **default** permite atÃ© **60 requisiÃ§Ãµes por minuto** por IP, adequado para rotas de consulta como listar sessÃµes ou verificar disponibilidade; e o **strict** permite apenas **5 requisiÃ§Ãµes por minuto** por IP, aplicado via decorator `@Throttle({ strict: { ttl: 60000, limit: 5 } })` nas rotas crÃ­ticas de reserva (`POST /bookings/reserve`) e confirmaÃ§Ã£o de pagamento (`POST /bookings/confirm`). Essa diferenÃ§a Ã© intencional: rotas que modificam estado e consomem recursos (locks Redis, transaÃ§Ãµes PostgreSQL, eventos Kafka) precisam de um limite mais agressivo para evitar que um Ãºnico IP sobrecarregue o sistema. Quando o limite Ã© excedido, a API retorna `HTTP 429 Too Many Requests`. Atualmente o rate limiting Ã© baseado exclusivamente no endereÃ§o IP do cliente; em uma evoluÃ§Ã£o futura, ele poderia ser combinado com autenticaÃ§Ã£o JWT para limitar por `userId`, oferecendo uma granularidade mais justa.

| Profile     | Limite            | Aplicado Em                                        |
| :---------- | :---------------- | :------------------------------------------------- |
| **default** | 60 req/min por IP | Todas as rotas (global)                            |
| **strict**  | 5 req/min por IP  | `POST /bookings/reserve`, `POST /bookings/confirm` |

---

## âš ï¸ LimitaÃ§Ãµes Conhecidas

| LimitaÃ§Ã£o                                         | Motivo                                                                                                                            |
| :------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------- |
| âŒ Dead Letter Queue (DLQ) nÃ£o implementada       | Priorizado: fluxo core primeiro. Mensagens com falha sÃ£o re-tentadas pelo Kafka, mas nÃ£o hÃ¡ fila separada para erros persistentes |
| âŒ MÃ©tricas (Prometheus/Grafana) nÃ£o configuradas | Escopo do desafio focava na lÃ³gica de concorrÃªncia                                                                                |
| âŒ Rate limiting apenas por IP                    | Uma soluÃ§Ã£o produÃ§Ã£o teria rate limit por `userId` via token JWT                                                                  |
| âŒ AutenticaÃ§Ã£o/AutorizaÃ§Ã£o nÃ£o implementada      | O `userId` Ã© passado no body. Em produÃ§Ã£o, viria de um JWT validado                                                               |
| âŒ Testes E2E com cobertura parcial               | Priorizados: cenÃ¡rios de concorrÃªncia e fluxo principal                                                                           |
| âŒ Dockerfile usa Node 18                         | Pode causar `crypto is not defined` em runtime â€” recomenda-se Node 20+                                                            |

---

## ğŸ§ª Testes

### Testes UnitÃ¡rios

Os testes unitÃ¡rios do projeto utilizam **Jest** como framework, que jÃ¡ vem integrado ao ecossistema NestJS. A estratÃ©gia de testes unitÃ¡rios foca em validar a lÃ³gica de negÃ³cio de forma isolada, **mockando todas as dependÃªncias externas** â€” ou seja, os acessos ao PostgreSQL (repositÃ³rios TypeORM), ao Redis (`RedisLockService`) e ao Kafka (`KafkaProducerService`) sÃ£o substituÃ­dos por mocks que simulam os comportamentos esperados sem precisar de infraestrutura real rodando. Isso garante que os testes sejam rÃ¡pidos (executam em milissegundos), determinÃ­sticos (nÃ£o dependem de estado externo) e possam ser rodados em qualquer ambiente, inclusive em pipelines de CI/CD. O mÃ³dulo `@nestjs/testing` Ã© utilizado para construir mÃ³dulos de teste com injeÃ§Ã£o de dependÃªncia, substituindo providers reais por implementaÃ§Ãµes mock. A cobertura alvo Ã© de **60-70%**, priorizando os caminhos crÃ­ticos como a lÃ³gica de reserva, validaÃ§Ã£o de assentos e confirmaÃ§Ã£o de pagamento.

```bash
# Rodar testes unitÃ¡rios
npm run test

# Rodar com cobertura
npm run test:cov

# Rodar em modo watch (re-executa a cada alteraÃ§Ã£o)
npm run test:watch
```

---

### Testes de IntegraÃ§Ã£o/ConcorrÃªncia

Os testes E2E (end-to-end) sÃ£o o pilar mais importante da suÃ­te de testes deste projeto, pois validam o **fluxo completo de reserva em cenÃ¡rios de concorrÃªncia real**. Utilizando `supertest` com o servidor NestJS levantado em memÃ³ria, os testes simulam mÃºltiplos usuÃ¡rios fazendo requisiÃ§Ãµes simultÃ¢neas para o **mesmo assento**, verificando que apenas **uma Ãºnica reserva** Ã© bem-sucedida enquanto as demais recebem `HTTP 409 Conflict`. O arquivo `bookings.e2e-spec.ts` configura um ambiente completo: cria um mÃ³dulo de teste a partir do `AppModule` real, desabilita o `ThrottlerGuard` (para nÃ£o bloquear requisiÃ§Ãµes durante o teste), inicializa o banco com `synchronize: true` para criar as tabelas automaticamente, e executa cenÃ¡rios que cobrem reserva bem-sucedida, rejeiÃ§Ã£o por assento indisponÃ­vel, confirmaÃ§Ã£o de pagamento, expiraÃ§Ã£o de reserva e histÃ³rico de compras. Esses testes provam que as 3 camadas de proteÃ§Ã£o (Redis Lock + PostgreSQL `FOR UPDATE` + IdempotÃªncia) funcionam corretamente em conjunto.

```bash
# Certifique-se de que a infraestrutura estÃ¡ rodando
docker-compose up -d postgres redis kafka zookeeper

# Executar testes E2E
npm run test:e2e
```

---

### Scripts de Teste

AlÃ©m dos testes automatizados com Jest, o projeto inclui **3 scripts bash** na pasta `scripts/` que permitem validar o sistema de forma manual e visual, especialmente Ãºteis para demonstrar o comportamento de concorrÃªncia em um ambiente real com Docker.

#### `scripts/seed-data.sh` â€” Popular Dados Iniciais

Este script cria uma sessÃ£o de cinema (filme "Oppenheimer", sala 1, 24 assentos, R$ 25,00) via `POST /api/sessions` e retorna o `SESSION_ID` gerado. Ã‰ o ponto de partida para qualquer teste manual â€” vocÃª precisa de uma sessÃ£o criada antes de poder reservar assentos.

```bash
bash scripts/seed-data.sh
```

#### `scripts/test-full-flow.sh` â€” Fluxo Completo de Compra

Executa o ciclo completo de compra em **8 passos automatizados**: (1) cria uma sessÃ£o, (2) verifica disponibilidade, (3) reserva 2 assentos, (4) confirma que os assentos estÃ£o reservados, (5) tenta reservar o mesmo assento com outro usuÃ¡rio e valida o `HTTP 409`, (6) confirma o pagamento, (7) verifica que os assentos mudaram para `sold`, e (8) consulta o histÃ³rico de compras. Esse script Ã© ideal para validar que **todo o fluxo funciona de ponta a ponta** apÃ³s subir o ambiente.

```bash
bash scripts/test-full-flow.sh
```

#### `scripts/concurrency-test.sh` â€” Teste de ConcorrÃªncia (10 UsuÃ¡rios SimultÃ¢neos)

Este Ã© o script mais importante para demonstrar a proteÃ§Ã£o contra race conditions. Ele dispara **10 requisiÃ§Ãµes HTTP simultÃ¢neas** (usando processos em background do bash) para reservar o **mesmo assento A1** da mesma sessÃ£o, cada uma com um `userId` diferente gerado via UUID. O resultado esperado Ã© que **exatamente 1** requisiÃ§Ã£o retorne `HTTP 201` (sucesso) e as outras **9** retornem `HTTP 409` (conflito). Se mais de 1 reserva for bem-sucedida, o script reporta falha de race condition. O script funciona em Linux, macOS e Windows (via Git Bash/WSL), gerando UUIDs de forma compatÃ­vel com cada plataforma.

```bash
# 1. Crie uma sessÃ£o e copie o SESSION_ID
bash scripts/seed-data.sh

# 2. Rode o teste de concorrÃªncia
bash scripts/concurrency-test.sh <SESSION_ID>
```

**Resultado esperado:**

```
Results:
  Successful: 1
  Conflicts: 9
  Errors: 0

TEST PASSED! Exactly 1 reservation succeeded (as expected)
```

> 10 requisiÃ§Ãµes simultÃ¢neas para o **mesmo assento** â†’ apenas **1** reserva bem-sucedida, 9 conflitos (HTTP 409).

---

## ğŸ”§ Troubleshooting

### Database "cinema" does not exist

Volume corrompido. Recrie:

```bash
docker-compose down -v
docker-compose up -d
```

### Kafka nÃ£o conecta

```bash
# Verificar logs
docker-compose logs kafka

# Reiniciar Kafka + Zookeeper
docker-compose restart zookeeper kafka
```

### PostgreSQL Connection Refused

```bash
# Verificar se estÃ¡ rodando
docker-compose ps postgres

# Acessar psql
docker exec -it cinema-postgres psql -U cinema -d cinema_booking
```

### Redis Lock Timeout

```bash
# Inspecionar locks ativos
docker exec -it cinema-redis redis-cli
> KEYS seat:lock:*
> DEL seat:lock:{sessionId}:{seatNumber}
```

### Erro `crypto is not defined` no Docker

O Dockerfile usa Node.js 18 que pode nÃ£o ter `crypto` global. Atualize para Node 20:

```dockerfile
FROM node:20-alpine AS builder
```

---

## ğŸ‘¤ Autor

**[Davi SIlva]**

- ğŸ“§ Email: xaviersilvadavi@gmail.com
- ğŸ”— LinkedIn: [meu-perfil](https://linkedin.com/in/davi-xavier-silva)

---

## ğŸ™ Agradecimentos

- Equipe **Starsoft** pela oportunidade e desafio tÃ©cnico

---
